\begin{abstract}
%Supervise learning is an effective method for building natural language processing (\NLP) models. 
The success of many natural language processing (\NLP) models is
bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question:
``Can we marry up a neural network (\NN) with regular expressions (\RE) to improve supervised learning for \NLP?". In answer, we develop
 novel methods to exploit the rich expressiveness of 
%the commonly used 
\RE at different levels within a \NN, showing that the combination
 significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach 
by applying it to \red{spoken} language understanding for intention detection and slot filling. Experiments show that our approach is
 highly effectively at exploiting the available training data, giving a clear boost to the vanilla, \RE-unaware \NN. %\z{Check the  abstract and title.}

%Combining neural networks (\NN) and human-generated rules can reduce the number of training examples required for learning an effective
%model, and can possibly produce better results than pure empirical methods. Specifically, we investigate the methods of combining \NN
%with regular expression (\RE), which is one of the most commonly used rules in natural language processing. We experiment our method in
%intent detection and slot filling settings with different amounts of training data, and the experimental results show that our methods
%give clear boost to the baseline \NN model by combining with \RE. Besides, we also give systematical analysis on the working scenario of
%each method, and on the influence of the complexity of the \RE as well.

\end{abstract}
