\section{Experimental Results}
\label{sec:experiments}

\subsection{Few-Shot Learning}
\RE patterns are often used for domains with limited training data in industry. Therefore, to answer question (1), we first explore the few-shot learning scenario.

\paragraph{Intent Detection}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy} \\
\hline
BLSTM & 45.28 / 60.02 & 60.62 / 64.61 & 63.60 / 80.52  \\
\hline
BLSTM+feat & 49.40 / 63.72 & 64.34 / 73.46 & 65.16 / 83.20   \\
\hline
BLSTM+logit & 46.01 / 58.68 & 63.51 / 77.83 & 69.22 / \textbf{89.25} \\
\hline
BLSTM+hu16 & 47.22 / 56.22 & 61.83 / 68.42 & 67.40 / 84.10  \\
\hline
two & 40.44 / 57.22 & 60.72 / 75.14 & 62.88 / 83.65  \\
\hline
two+posi & 50.90 / 74.47 & 68.69 / 84.66 & 72.43 / 85.78  \\
\hline
two+neg & 49.01 / 68.31 & 64.67 / 79.17 & 72.32 / 86.34   \\
\hline
two+both & \textbf{54.86} / \textbf{75.36} & \textbf{71.23} / \textbf{85.44} & \textbf{75.58} / 88.80   \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data}
\label{tab_intent_few}
\end{table}

As shown in Table \ref{tab_intent_few}, all the methods makes improvements to the baseline \texttt{BLSTM}, and using \RE to guide the attention module works best. Specifically, \texttt{posi} performs better than \texttt{neg} when the data is limited, which is because that the negative patterns are derived from the positive ones with some noise. However, \texttt{neg} works slightly better in the 20-shot setting, which is possibly due to that negative patterns significantly outnumbers the positive ones, and this also indicates the noise tolerence ability of \NN. Further, not surprisingly, we get the best results when positive and negative patterns are combined.

Further, we can also see that \texttt{logit} works generally better than \texttt{+feature}, except for the 5-shot case. This is probably because that adding a value directly to the logit of each intent is more clear and straightforward than using the average of embeddings of the intent predicted by \RE as feature. And the straightforwardness of \texttt{logit} also makes the final prediction more sensitive to the noise of \RE, which lead to worse results in the 5-shot setting.

Another pattern that we can recognize from Table \ref{tab_intent_few} is that, the improvement from \RE decreases as the size of training data increases, which is reasonable since there is more overlap between the information contained by the data and those contained by \RE as we have more data. Besides, \texttt{two} ourperforms the pure \BLSTM from the 10-shot setting even without guidance from \RE, showing the effectiveness the two-side attention architecture.

Besides, \texttt{two+both} significantly outperforms the results produced by our \RE (see the \RE row in Table \ref{tab_full}) since the 10-shot setting, indicating that our method do help generalization the \RE patterns by fusing them with \NN. 


\paragraph{Slot Filling}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  &  \multicolumn{3}{|c|}{Macro-F1 / Micro-F1}   \\
\hline
BLSTM & 60.78 / 83.91 & 74.28  / 90.19 & 80.57 / 93.08 \\
\hline
BLSTM+feat & \textbf{66.84} / \textbf{88.96} & 79.67 / \textbf{93.64} & 84.95 / 95.0 \\
\hline
BLSTM+logit & 63.68 / 86.18 & 76.12 / 91.64  & 83.71 / 94.43 \\
\hline
BLSTM+hu16 & 63.37 / 85.37 & 75.67 / 91.06 & 80.85 / 93.47 \\
\hline
two & 59.4 / 83.29 & 73.22 / 90.08 & 79.58 / 92.57 \\
\hline
two+posi & 59.81 / 81.0 & 73.62 / 89.28 & 78.94 / 92.21 \\
% \hline
% two+posi+feat & 66.08 / 87.57 & \textbf{81.39} / 93.0 & 84.11 / 94.92\\
\hline
mem & 61.25 / 83.45 & 77.83 / 90.57 & 82.98 / 93.49 \\
\hline
mem+feat & 65.08 / 88.07 & \textbf{80.64} / 93.47 & \textbf{85.45} / \textbf{95.39} \\
\hline
\end{tabular}
}
\caption{Slot Filling Result on Few-Shot Data}
\label{tab_slot_few}
\end{table}

Different from intent detection, as shown in Table \ref{tab_slot_few}, the attention loss does not work for slot filling.
% \footnote{Negative pattern works even worse, we therefore only show the results of positive pattern here}. 
The reason is that, the major signal that decides the tag of a word is the word itself, together with 0-3 words in the context.
However, attention does not contribute to better self-awareness. 
Therefore, since \BLSTM already has the ability to model the context words, the additional attention loss is more of a burden than benifit.
In our further experiment where we add the attention module to \BLSTM but without attention loss to train on the full dataset. 
We find that instead of marking informative context words, the attention tend to concentrate on the word that is to be predicted itself, which confirms our hypothesis about why attention loss does not work. 
Note that, since negative attention performs even worse, we only show positive attention results here.

Another difference is that, although both \texttt{logit} and \texttt{feat} work, \texttt{feat} performs better this time. The reason is probably that, rather than simply adding a value to the logit of each label, \texttt{feat} introduces more information about the target word itself, which helps LSTM make better understanding of the sentence.

% It is also interesting that, \texttt{posi} and \texttt{feat} is complementary to each other. \texttt{posi+feat} performs better than pure \texttt{feat} itselft when we have enough data. This is probably because the extra light-weight NER from \texttt{feat} significantly reduces the need for self-awareness of the target word, which leaves possibility to better model the context (using attention).

Besides, we can see that even our vanila Bi-LSTM model outperforms the pure \RE output (see the \RE row in Table \ref{tab_full}) in 5-shot, showing that it is hard to write high-performance \RE patterns, but using \RE to boost \NN models is feasible. 
% It is not surprising because this is not true 5-shot setting, extra data still exists for frequent patterns since one sentence may contain multiple slots.


\subsection{Compare with Memory Network}
To better understand the relationship between our method and other existing few-shot learning methods, we also implement the memory network method \cite{kaiser2017learning}, which achieves good results in various few-shot settings. Specifically, by adapting their open-source code, we add their proposed memory module to our \BLSTM model.

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy}   \\
\hline
BLSTM & 64.73 / 91.71 & 78.55 / 96.53 & 82.05 / 97.20 \\
\hline
BLSTM+hu16 & 65.22 / 91.94 & 84.49 / 96.75 & 84.80 / 97.42 \\
\hline
two & 65.59 / 91.04 & 77.92 / 95.52 & 81.01 / 96.86 \\
\hline
two+both & 66.62 / 92.05 & 85.75 / 96.98 & \textbf{87.97} / \textbf{97.76} \\
\hline
mem & 67.54 / 91.83 & 82.16 / 96.75 & 84.69 / 97.42 \\
\hline
mem+posi & \textbf{70.46} / \textbf{93.06} & \textbf{86.03} / \textbf{97.09} & 86.69 / 97.65 \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data with the Top 3 Intents Having 300 Sentences.}
\label{tab_intent_few_fill}
\end{table}

Since the memory module requires either many few-shot classes or several classes with more data, we expand our few-shot learning dataset for intent detection, so that the top 3 intents have 300 sentences. As shown in Table \ref{tab_intent_few_fill}, while the memory module works better than the base model, our attention loss makes clear improvements in this setting. Further, the attention loss can also be combined with the memory module (\texttt{mem+posi}), and achieves better results than using the memory module alone. Note that, since the attention module requires the input sentence to have only one embedding, we only use the possitive attention loss to guide the attention module in \BLSTM.

As for slot filling, since we already have extra data for frequent tags in the original few-shot setting, we use them to run the memory module directly. As can be seen in Table \ref{tab_slot_few}, the memory module also generally improves the base model, and it is also compatible to our \texttt{feat} fusion method, which gives \texttt{mem} a further boost.

Note that, for the compactness of the paper, we only combine the best-performant fusion method in each task with \texttt{mem}. Other methods in each task can also be easily combined as well. 

% NOTE: since we have more data here, the importance of attention guidance is not that crucial as in the previous more strict few-shot leanring setting. Therefore, the weight of attention loss is reduced to 1 in this setting and the following full dataset setting, 


\subsection{Full Dataset}
To answer question (2), we also experiment our methods on the full dataset, 

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
 & Intent & Slot \\ 
\hline
  & Macro-F1 / Accuracy &  Macro-F1 / Micro-F1 \\
\hline
BLSTM & 92.50 / 98.77  & 85.01 / 95.47\\
\hline
BLSTM+feat & 87.36 / 95.97 & 86.7 / 95.55\\
\hline
BLSTM+logit & 92.48 / 98.77 & 86.94 / 95.42  \\
\hline
BLSTM+hu16 & 93.09 / 98.77 & 85.74 / 95.33  \\
\hline
two & 93.64 / 98.88  & 84.45 / 95.05\\
% \hline
% two+posi & 93.02 & 98.88 \\
% \hline
% two+neg & 93.92 & \textbf{98.99} \\
\hline
two+both/posi & \textbf{96.20} / \textbf{98.99} & 85.37 / 95.34 \\
\hline
mem & 93.42 / 98.77 & 85.72 / 95.37\\
\hline
mem+posi/feat & 94.36 / \textbf{98.99} & \textbf{87.82} / \textbf{95.90} \\
\hline
\hline
RE & 70.31 / 68.98 & 42.33 / 70.79\\
\hline
L\&L16 & - / 98.43 & - / 95.98\\
\hline 

\end{tabular}
}
\caption{Results on Full Dataset. The left side of \texttt{two+both/posi} and \texttt{mem+posi/feat} applies for intent, and the right side applies for slot.} 
\label{tab_full}
\end{table}

Table \ref{tab_full} shows the results for intent detection and slot filling on the full dataset. We can see that, for slot filling, the attention loss still works, \texttt{logit} achieves similar results to \texttt{BLSTM}, but \texttt{feat} performs significantly worse than the baseline. This shows that, although the informative words marked by \RE still helps, the low-performance \RE output no longer works when the labeled data is sufficient. While the \NN can learn to assign low weights for the \RE output in \texttt{logit}, the noise coming from the input feature is hard to eliminate, and theirfore leading to bad results of \texttt{feat}.

As for slot filling, while the attention loss still does not work, \texttt{feat} and \texttt{logit} both works as in the few-shot setting. The reason is probably that the output of the \RE used for \texttt{feat} and \texttt{logit} is only the entity part of the slot label, which is only indirectly connected to the prediction target. The noise contained in the indirectness makes the \NN relies less on the \RE output, and therefore is less sensitive to the wrong predictions made by \RE as they do in the intent detection scenario.

Further, we can see that \texttt{mem} still generally works, and we can also make improvements by combining our fusion methods. Besides, the state-of-art results on the ATIS data produced by \cite{liu2016attention} is also included (\LL). We can see that our base \BLSTM model achieves comparative results to \LL, confirming that the improvements from our fusion method does not come from the inferior ability of the base model\footnote{Since slot filling is evaluated in phrase level, there is almost no difference in F1 when we convert the prediction on our split data format to the original format (see Section \ref{sec_datasest}), making our results still comparatable to \LL.}.

\subsection{Complexity of the Pattern}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|c|}

\hline
  & \multicolumn{2}{|c|}{Intent} & \multicolumn{2}{|c|}{Slot}  \\
\hline
  & \multicolumn{2}{|c|}{Macro-F1 / Accuracy} & \multicolumn{2}{|c|}{Macro-F1 / Micro-F1}  \\
\hline
  & Before & After & Before & After \\
\hline
base & \multicolumn{2}{|c|}{63.60 / 80.52} & \multicolumn{2}{|c|}{80.57 / 93.08}  \\
\hline
+feat & 65.16/\textbf{83.20} & \textbf{66.51}/80.40 & \textbf{84.95/95.00} & 83.88/94.71 \\
\hline
+logit & \textbf{69.22/89.25} & 65.09/83.09 & \textbf{83.71/94.43} & 83.22/93.94  \\
\hline
+both & \textbf{75.58/88.80} & 74.51/87.46 & - & - \\
\hline 
\end{tabular}
}
\caption{Results on 20-Shot Data with Simple Patterns. \texttt{base} is \texttt{BLSTM}, the base model for \texttt{+feat} and \texttt{+logit}. \texttt{+both} refers to \texttt{two+both}.}
\label{tab_simple}
\end{table}

Since enriching the number of phrases in a group with $|$ is usually easier than designing the structure of the pattern (i.e., arranging the groups), we choose to modify the number of groups in a pattern to see the relationship between pattern complexity and performance.

Specifically, for intent detection, we simplify all the patterns with more than 2 meaningful groups (except for groups that match a sequence of any words), so that only 2 meaningful groups are kept. In total, 17 patterns are simplfied. 
As for slot filling, since the number of patterns with more than 2 meaningful groups is limited (only 7 patterns), we choose a more aggresive strategy that all the patterns are reduced to word list pattern, except for patterns whose main group matches a number or a common word, which is not distinguishable at all on its own. Further, all the remaining patterns are ensured to have at most 2 meaningful groups. In total, 21 patterns are affected.

As shown in Table \ref{tab_simple}, simplifying the \RE generally results in worse performance, but the results are still better than the base \BLSTM. This shows that, while the quality of the \RE matters when fusing with \NN, we can still get useful information from low-quality patterns. Further, we can see that the attention loss is much less sensitive to the simplification than other methods. The reason probably lies in that, although simplified, the core informative words still remains in the pattern, and is therefore still helpful for the attention module.\footnote{We do not include the simplification results for attention loss since the \RE used for it is different from the other two methods, and we already know it does not work for slot filling} 






