\section{Experimental Results}
\label{sec:experiments}

\subsection{Few-Shot Learning}
\RE patterns are often used for domains with limited training data in industry. Therefore, to answer question (1), we first explore the few-shot learning scenario.

\paragraph{Intent Detection}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy} \\
\hline
BLSTM & 45.28 / 60.02 & 60.62 / 64.61 & 63.60 / 80.52  \\
\hline
BLSTM+feat & 49.40 / 63.72 & 64.34 / 73.46 & 65.16 / 83.20   \\
\hline
BLSTM+logit & 46.01 / 58.68 & 63.51 / 77.83 & 69.22 / \textbf{89.25} \\
\hline
two & 40.44 / 57.22 & 60.72 / 75.14 & 62.88 / 83.65  \\
\hline
two+posi & 50.90 / 74.47 & 68.69 / 84.66 & 72.43 / 85.78  \\
\hline
two+neg & 49.01 / 68.31 & 64.67 / 79.17 & 72.32 / 86.34   \\
\hline
two+both & \textbf{54.86} / \textbf{75.36} & \textbf{71.23} / \textbf{85.44} & \textbf{75.58} / 88.80   \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data}
\label{tab_intent_few}
\end{table}

As shown in Table \ref{tab_intent_few}, all the methods makes improvements to the baseline \texttt{BLSTM}, and using \RE to guide the attention module works best. Specifically, \texttt{posi} performs better than \texttt{neg} when the data is limited, which is because that the negative patterns are derived from the positive ones with some noise. However, \texttt{neg} works slightly better in the 20-shot setting, which is possibly due to that negative patterns significantly outnumbers the positive ones, and this also indicates the noise tolerence ability of \NN. Further, not surprisingly, we get the best results when positive and negative patterns are combined.

Further, we can also see that \texttt{logit} works generally better than \texttt{+feature}, except for the 5-shot case. This is probably because that adding a value directly to the logit of each intent is more clear and straightforward than using the average of embeddings of the intent predicted by \RE as feature. And the straightforwardness of \texttt{logit} also makes the final prediction more sensitive to the noise of \RE, which lead to worse results in the 5-shot setting.

Another pattern that we can recognize from Table \ref{tab_intent_few} is that, the improvement from \RE decreases as the size of training data increases, which is reasonable since there is more overlap between the information contained by the data and those contained by \RE as we have more data. Besides, \texttt{two} ourperforms the pure \BLSTM from the 10-shot setting even without guidance from \RE, showing the effectiveness the two-side attention architecture.

Besides, \texttt{two+both} significantly outperforms the results produced by our \RE (see the \RE row in Table \ref{tab_intent_full}) since the 10-shot setting, indicating that our method do help generalization the \RE patterns by fusing them with \NN. 


\paragraph{Slot Filling}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  &  \multicolumn{3}{|c|}{Macro-F1 / Micro-F1}   \\
\hline
BLSTM & 60.78 / 83.91 & 74.28  / 90.19 & 80.57 / 93.08 \\
\hline
BLSTM+feat & \textbf{66.84} / \textbf{88.96} & 79.67 / \textbf{93.64} & 84.95 / 95.0 \\
\hline
BLSTM+logit & 63.68 / 86.18 & 76.12 / 91.64  & 83.71 / 94.43 \\
\hline
two & 59.4 / 83.29 & 73.22 / 90.08 & 79.58 / 92.57 \\
\hline
two+posi & 59.81 / 81.0 & 73.62 / 89.28 & 78.94 / 92.21 \\
% \hline
% two+posi+feat & 66.08 / 87.57 & \textbf{81.39} / 93.0 & 84.11 / 94.92\\
\hline
mem & 61.25 / 83.45 & 77.83 / 90.57 & 82.98 / 93.49 \\
\hline
mem+feat & 65.08 / 88.07 & 80.64 / 93.47 & \textbf{85.45} / \textbf{95.39} \\
\hline
\end{tabular}
}
\caption{Slot Filling Result on Few-Shot Data}
\label{tab_slot_few}
\end{table}

Different from intent detection, as shown in Table \ref{tab_slot_few}, the attention loss does not work for slot filling.
% \footnote{Negative pattern works even worse, we therefore only show the results of positive pattern here}. 
The reason is that, the major signal that decides the tag of a word is the word itself, together with 0-3 words in the context.
However, attention does not contribute to better self-awareness. 
Therefore, since \BLSTM already has the ability to model the context words, the additional attention loss is more of a burden than benifit.
In our further experiment where we add the attention module to \BLSTM but without attention loss to train on the full dataset. 
We find that instead of marking informative context words, the attention tend to concentrate on the word that is to be predicted itself, which confirms our hypothesis about why attention loss does not work. 
Note that, since negative attention performs even worse, we only show positive attention results here.

Another difference is that, although both \texttt{logit} and \texttt{feat} work, \texttt{feat} performs better this time. The reason is probably that, rather than simply adding a value to the logit of each label, \texttt{feat} introduces more information about the target word itself, which helps LSTM make better understanding of the sentence.

% It is also interesting that, \texttt{posi} and \texttt{feat} is complementary to each other. \texttt{posi+feat} performs better than pure \texttt{feat} itselft when we have enough data. This is probably because the extra light-weight NER from \texttt{feat} significantly reduces the need for self-awareness of the target word, which leaves possibility to better model the context (using attention).

Besides, we can see that even our vanila Bi-LSTM model outperforms the pure \RE output (see the \RE row in Table \ref{tab_slot_full}) in 5-shot, showing that it is hard to write high-performance \RE patterns, but using \RE to boost \NN models is feasible. 
% It is not surprising because this is not true 5-shot setting, extra data still exists for frequent patterns since one sentence may contain multiple slots.


\subsection{Compare with Memory Network}
To better understand the relationship between our method and the existing few-shot learning methods, we also implement the memory network model \cite{kaiser2017learning}, which achieves good results in various datasets. Since it proposes a memory module that is plugable to various model, we use the memory module from their released code and adpat it to our intent detection and slot filling base models.

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy}   \\
\hline
raw & 64.73 / 91.71 & 78.55 / 96.53 & 82.05 / 97.20 \\
\hline
two & 65.59 / 91.04 & 77.92 / 95.52 & 81.01 / 96.86 \\
\hline
two+both & 66.62 / 92.05 & 85.75 / 96.98 & \textbf{87.97} / \textbf{97.76} \\
\hline
mem & 67.54 / 91.83 & 82.16 / 96.75 & 84.69 / 97.42 \\
\hline
mem+posi & \textbf{70.46} / \textbf{93.06} & \textbf{86.03} / \textbf{97.09} & 86.69 / 97.65 \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data with the Top 3 Intents Having 300 Sentences.}
\label{tab_intent_few_fill}
\end{table}

See Table \ref{tab_intent_few_fill}, the top 3 labels have 300 sentences. Since we already have extra data for frequent tags in few-shot slot filling setting, we use them to run memory network directly. Further, since the memory module require the input datum to have only one embedding, we therefore simplify our multi-attention model for intent detection, and only use one set of attention to generate only one sentence embedding. Consequently, we only use positive attention in this setting.

The memory module generally improves the base model, and it is also compatible to our proposed method, which receives significant improvement when combined with \RE patterns.

NOTE: since we have more data here, the importance of attention guidance is not that crucial as in the previous more strict few-shot leanring setting. Therefore, the weight of attention loss is reduced to 1 in this setting and the following full dataset setting, 


\subsection{Full Dataset}
To see if the \RE patterns still helps when data is abundant, we also experiment our methods on the full dataset.

\paragraph{Intent Detection}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Accuracy  \\
\hline
raw & 89.55 / 95.66 / 92.50 & 98.77  \\
\hline
raw+feat & 94.52 / 81.22 / 87.36 & 95.97  \\
\hline
raw+logit & 89.50 / 95.66 / 92.48 & 98.77  \\
\hline
two & 90.83 / 96.61 / 93.64 & 98.88 \\
\hline
two+posi & 89.68 / 96.61 / 93.02 & 98.88 \\
\hline
two+neg & 91.35 / 96.63 / 93.92 & \textbf{98.99} \\
\hline
two+both & \textbf{95.78} / \textbf{96.63} / \textbf{96.20} & \textbf{98.99} \\
\hline
\hline
RE & 84.37 / 60.27 / 70.31 & 68.98 \\
\hline
mem & 92.31 / 94.56 / 93.42 & 98.77 \\
\hline
mem+posi & 91.08 / 97.88 / 94.36 & 98.99 \\
\hline
joint & - & 98.43 \\
\hline 

\end{tabular}
}
\caption{Intent Detection Result on Full Dataset}
\label{tab_intent_full}
\end{table}

See Tabel \ref{tab_intent_full}.
Our model achieves comparitive results to state-of-art (\texttt{joint}). The higher results of our base model probably comes from the extra pretrained vector, and the preprocessing step that separating \emph{'s} from a noun. We can still see clear improvements by using two-side attention. The memory network it self increases macro F1 but with slightly lower overall accuracy. 


\paragraph{Slot Filling}
\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Micro-P/R/F1  \\
\hline
raw &  84.67 / 85.35 / 85.01            & 95.04 / 95.91 / 95.47  \\
\hline
raw+feat & \textbf{87.8} / \textbf{86.1} / \textbf{86.94} & 95.03 / 95.8 / 95.42  \\
\hline
raw+logit & 87.43 / 85.98 / 86.7        & 95.27 / 95.84 / 95.55  \\
\hline
two & 83.96 / 84.94 / 84.45             & 94.65 / 95.45 / 95.05 \\
\hline
two+posi & 86.51 / 84.26 / 85.37        & 94.61 / 95.34 / 95.34  \\
\hline
two+posi+feat & 85.32 / 85.74 / 85.53   & \textbf{95.58} / \textbf{96.19} / \textbf{95.89} \\
\hline
\hline
RE & 52.56 / 35.44 / 42.33 & 82.37 / 62.07 / 70.79 \\
\hline
mem & 86.21 / 85.23 / 85.72 & 94.93 / 95.8 / 95.37 \\
\hline
mem+feat & 88.21 / 87.43 / 87.82 & 95.62 / 96.19 / 95.9 \\
\hline
joint & - & - / - / \textbf{95.98} \\
\hline 

\end{tabular}
}
\caption{Slot Filling Result on Full Dataset}
\label{tab_slot_full}
\end{table}

See Table \ref{tab_slot_full}. The base model achieves comparative results to state-of-art (\texttt{joint}).
\texttt{feat} and \texttt{logit} also works. 


\subsection{Complexity of the Pattern}
Since, enriching a group is usually easier than designing the structure of the pattern (arranging the groups), we choose to modify the number of groups in a pattern to see the relationship between pattern complexity and performance.

Specifically, for intent detection, we simplify all the patterns with more than 2 meaningful groups (except for groups that match all the words), so that they will only have 2 meaningful groups. In total, 17 patterns are simplfied. The results are shown in Table \ref{simple_intent}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-F1 & Accuracy  \\
\hline
raw & 63.60 & 80.52 \\
\hline
raw+feat & 65.16 $\rightarrow$ 66.51 & 83.20 $\rightarrow$ 80.40 \\
\hline
raw+logit & 69.22 $\rightarrow$ 65.09 & 89.25 $\rightarrow$ 83.09  \\
\hline
two+both & 75.58 $\rightarrow$ 74.51 & 88.80 $\rightarrow$ 87.46 \\
\hline 
\end{tabular}
}
\caption{Intent Detection Results on 20-Shot Data with Simple Patterns}
\label{simple_intent}
\end{table}


As for slot filling, since the number of patterns with more than 2 meaningful groups is limited (only 7 patterns), we choose a more aggresive strategy that all the patterns are reduced to word list pattern, except for patterns whose major component is a number, which is not distinguishable at all on their own, or the word is a common word itself. Further, all the remaining patterns are ensured to have at most 2 meaningful groups. In total, 21 patterns are affected. See Table \ref{simple_slot}.

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Micro-P/R/F1  \\
\hline
raw & 80.57 & 93.08 \\
\hline
raw+feat & 84.95 $\rightarrow$ 83.88 & 95.00 $\rightarrow$ 94.71 \\
\hline
raw+logit & 83.71 $\rightarrow$ 83.22 & 94.43 $\rightarrow$ 93.94  \\
\hline 
\end{tabular}
}
\caption{Slot Filling Results on 20-Shot Data with Simple Patterns}
\label{simple_slot}
\end{table}




