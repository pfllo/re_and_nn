\section{Experimental Results}
\label{sec:experiments}

\subsection{Few-Shot Learning}
\RE patterns are often used for domains with limited trainging data in industry. Therefore, we first explore whether combining \RE patterns improves \NN models, and how does it compare to pure \RE patterns.

\paragraph{Intent Detection}

\begin{table*}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
  & \multicolumn{2}{|c|}{5-shot} & \multicolumn{2}{|c|}{10-shot} & \multicolumn{2}{|c|}{20-shot}  \\
 \hline
  & Macro-P/R/F1 & Accuracy & Macro-P/R/F1 & Accuracy  & Macro-P/R/F1 & Accuracy   \\
\hline
raw & 37.36 / 57.44 / 45.28 & 60.02 & 50.84 / 75.07 / 60.62 & 64.61 & 56.01 / 73.58 / 63.60 & 80.52  \\
\hline
raw+feat & 44.25 / 55.92 / 49.40 & 63.72 & 57.90 / 72.39 / 64.34 & 73.46 & 57.09 / 75.89 / 65.16 & 83.20   \\
\hline
raw+logit & 38.11 / 58.06 / 46.01 & 58.68 & 55.39 / 74.41 / 63.51 & 77.83 & 62.67 / 77.31 / 69.22 & \textbf{89.25} \\
\hline
two & 33.65 / 50.67 / 40.44 & 57.22 & 51.83 / 73.29 / 60.72 & 75.14 & 57.09 / 69.98 / 62.88 & 83.65  \\
\hline
two+posi & 43.87 / 60.61 / 50.90 & 74.47 & 59.15 / 81.90 / 68.69 & 84.66 & 63.08 / 85.02 / 72.43 & 85.78  \\
\hline
two+neg & 41.92 / 58.99 / 49.01 & 68.31 & 56.29 / 75.98 / 64.67 & 79.17 & 63.65 / 83.73 / 72.32 & 86.34   \\
\hline
two+both & \textbf{47.89 / 64.22 / 54.86} & \textbf{75.36} & \textbf{61.80 / 84.06 / 71.23} & \textbf{85.44} & \textbf{68.02 / 85.03 / 75.58} & 88.80   \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data}
\label{tab_intent_few}
\end{table*}
See Table \ref{tab_intent_few}. 

All the methods works, attention perfoms best, both positive and negative works, and it perfoms best when they are combined together.

\texttt{logit} is better than \texttt{feature}, probably because the information from \texttt{logit} is more clear (one logit value per intent), and \texttt{feature} is the mix of prediction of various patterns (average of matched intent vector). They does not perform well when training data is limited (5-shot), because of extra learnable parameters.

The improvement from \RE decreases as the size of training data increases.

Our two-side attention ourperforms the pure \RE output (see the \RE row in Table \ref{tab_intent_full}) in 10-shot setting. 


\paragraph{Slot Filling}

\begin{table*}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
  & \multicolumn{2}{|c|}{5-shot} & \multicolumn{2}{|c|}{10-shot} & \multicolumn{2}{|c|}{20-shot}  \\
 \hline
  & Macro-P/R/F1 & Micro-P/R/F1 & Macro-P/R/F1 & Micro-P/R/F1  & Macro-P/R/F1 & Micro-P/R/F1   \\
\hline
raw & 59.83/61.76/60.78 & 84.04/83.77/83.91 & 73.1/75.51/74.28  & 90.08/90.3/90.19 & 80.04/81.1/80.57 & 92.87/93.3/93.08 \\
\hline
raw+feat & \textbf{67.66/66.04/66.84} & \textbf{90.24/87.72/88.96} & 80.75/78.61/79.67 & \textbf{94.6/92.7/93.64} & 83.56/82.92/83.24 & 94.81/94.74/94.78 \\
\hline
raw+logit & 63.9/63.47/63.68 & 86.93/85.43/86.18 & 76.02/76.23/76.12 & 91.82/91.46/91.64  & 83.3/84.11/83.71 & 94.14/94.71/94.43 \\
\hline
two & 57.48/61.45/59.4 & 82.91/83.67/83.29 & 72.5/73.95/73.22 & 90.32/89.84/90.08 & 77.98/81.25/79.58 & 92.44/92.7/92.57 \\
\hline
two+posi & 58.08/61.66/59.81 & 81.26/80.74/81.0 & 73.38/73.87/73.62 & 89.88/88.68/89.28 & 77.91/80.01/78.94 & 92.12/92.31/92.21 \\
\hline
two+posi+feat & 66.51/65.65/66.08 & 88.42/86.74/87.57 & \textbf{80.89/81.89/81.39} & 93.48/92.52/93.0 & 84.41/83.8/84.11 & 95.02/94.81/94.92\\
\hline
mem & 61.85/60.67/61.25 & 83.4/83.49/83.45 & 77.52/78.15/77.83  & 90.49/90.65/90.57 & 82.94/83.02/82.98 & 93.29/93.69/93.49 \\
\hline
mem+feat & 65.4/64.76/65.08 & 89.07/87.09/88.07 & 80.47/80.81/80.64 & 94.26/92.7/93.47 & \textbf{86.24/84.67/85.45} & \textbf{95.44/95.34/95.39} \\
\hline
\end{tabular}
}
\caption{Slot Filling Result on Few-Shot Data}
\label{tab_slot_few}
\end{table*}
See Table \ref{tab_slot_few}.

Attention does not work, because the major signal that decides the tag of a word is the word itself, together with 0-3 words in the context. However, attention does not contribute to better self-awareness, and LSTM already has the ability to model context word, the additional attention loss is more of a burden than benifit. Negative attention performs worse, so we only include the results for positive attention in Table \ref{tab_slot_few}.

Contrary to intent detection, although \texttt{logit} and \texttt{feat} both works, \texttt{feat} performs better this time. The reason is \texttt{feat} introduces more information about the target word itself, which makes LSTM make better understanding of the sentence.

It is interesting that positive attention and \texttt{feat} is complementary to each other (but the combination does not work for intent detection). \texttt{posi+feat} performs better than pure \texttt{feat} itselft when we have enough data (see 10-shot and 20-shot). This is probably because the extra light-weight NER from \texttt{feat} significantly reduces the need for self-awareness of the target word, which leaves possibility to better model the context (using attention).

Even our vanila Bi-LSTM model out performs the pure \RE output (see the \RE row in Table \ref{tab_slot_full}) in 5-shot. It is not surprising because this is not true 5-shot setting, extra data still exists for frequent patterns since one sentence may contain multiple slots.


\subsection{Compare with Memory Network}
To better understand the relationship between our method and the existing few-shot learning methods, we also implement the memory network model \cite{kaiser2017learning}, which achieves good results in various datasets. Since it proposes a memory module that is plugable to various model, we use the memory module from their released code and adpat it to our intent detection and slot filling base models.

\begin{table*}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
  & \multicolumn{2}{|c|}{5-shot} & \multicolumn{2}{|c|}{10-shot} & \multicolumn{2}{|c|}{20-shot}  \\
 \hline
  & Macro-P/R/F1 & Accuracy & Macro-P/R/F1 & Accuracy  & Macro-P/R/F1 & Accuracy   \\
\hline
raw & 68.18/61.61/64.73 & 91.71 & 78.63/78.48/78.55 & 96.53 & 80.22/83.96/82.05 & 97.20 \\
\hline
two & 67.96/63.38/65.59 & 91.04 & 77.64/78.20/77.92 & 95.52 & 79.57/82.50/81.01 & 96.86 \\
\hline
two+both & 67.68/65.59/66.62 & 92.05 & \textbf{84.54}/86.99/85.75 & 96.98 & \textbf{87.78}/88.16/\textbf{87.97} & \textbf{97.76} \\
\hline
mem & 71.38/64.09/67.54 & 91.83 & 79.43/85.09/82.16 & 96.75 & 82.10/87.45/84.69 & 97.42 \\
\hline
mem+posi & \textbf{75.08/66.37/70.46} & \textbf{93.06} & 84.26/\textbf{87.88/86.03} & \textbf{97.09} & 83.59/\textbf{90.03}/86.69 & 97.65 \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data with the Top 3 Intents Having 300 Sentences.}
\label{tab_intent_few_fill}
\end{table*}
See Table \ref{tab_intent_few_fill}, the top 3 labels have 300 sentences. Since we already have extra data for frequent tags in few-shot slot filling setting, we use them to run memory network directly. Further, since the memory module require the input datum to have only one embedding, we therefore simplify our multi-attention model for intent detection, and only use one set of attention to generate only one sentence embedding. Consequently, we only use positive attention in this setting.

The memory module generally improves the base model, and it is also compatible to our proposed method, which receives significant improvement when combined with \RE patterns.

NOTE: since we have more data here, the importance of attention guidance is not that crucial as in the previous more strict few-shot leanring setting. Therefore, the weight of attention loss is reduced to 1 in this setting and the following full dataset setting, 


\subsection{Full Dataset}
To see if the \RE patterns still helps when data is abundant, we also experiment our methods on the full dataset.

\paragraph{Intent Detection}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Accuracy  \\
\hline
raw & 89.55 / 95.66 / 92.50 & 98.77  \\
\hline
raw+feat & 94.52 / 81.22 / 87.36 & 95.97  \\
\hline
raw+logit & 89.50 / 95.66 / 92.48 & 98.77  \\
\hline
two & 90.83 / 96.61 / 93.64 & 98.88 \\
\hline
two+posi & 89.68 / 96.61 / 93.02 & 98.88 \\
\hline
two+neg & 91.35 / 96.63 / 93.92 & \textbf{98.99} \\
\hline
two+both & \textbf{95.78} / \textbf{96.63} / \textbf{96.20} & \textbf{98.99} \\
\hline
\hline
RE & 84.37 / 60.27 / 70.31 & 68.98 \\
\hline
mem & 92.31 / 94.56 / 93.42 & 98.77 \\
\hline
mem+posi & 91.08 / 97.88 / 94.36 & 98.99 \\
\hline
joint & - & 98.43 \\
\hline 

\end{tabular}
}
\caption{Intent Detection Result on Full Dataset}
\label{tab_intent_full}
\end{table}

See Tabel \ref{tab_intent_full}.
Our model achieves comparitive results to state-of-art (\texttt{joint}). The higher results of our base model probably comes from the extra pretrained vector, and the preprocessing step that separating \emph{'s} from a noun. We can still see clear improvements by using two-side attention. The memory network it self increases macro F1 but with slightly lower overall accuracy. 


\paragraph{Slot Filling}
\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Micro-P/R/F1  \\
\hline
raw &  84.67 / 85.35 / 85.01            & 95.04 / 95.91 / 95.47  \\
\hline
raw+feat & \textbf{87.8} / \textbf{86.1} / \textbf{86.94} & 95.03 / 95.8 / 95.42  \\
\hline
raw+logit & 87.43 / 85.98 / 86.7        & 95.27 / 95.84 / 95.55  \\
\hline
two & 83.96 / 84.94 / 84.45             & 94.65 / 95.45 / 95.05 \\
\hline
two+posi & 86.51 / 84.26 / 85.37        & 94.61 / 95.34 / 95.34  \\
\hline
two+posi+feat & 85.32 / 85.74 / 85.53   & \textbf{95.58} / \textbf{96.19} / \textbf{95.89} \\
\hline
\hline
RE & 52.56 / 35.44 / 42.33 & 82.37 / 62.07 / 70.79 \\
\hline
mem & 86.21 / 85.23 / 85.72 & 94.93 / 95.8 / 95.37 \\
\hline
mem+feat & 88.21 / 87.43 / 87.82 & 95.62 / 96.19 / 95.9 \\
\hline
joint & - & - / - / \textbf{95.98} \\
\hline 

\end{tabular}
}
\caption{Slot Filling Result on Full Dataset}
\label{tab_slot_full}
\end{table}

See Table \ref{tab_slot_full}. The base model achieves comparative results to state-of-art (\texttt{joint}).
\texttt{feat} and \texttt{logit} also works. 


\subsection{Complexity of the Pattern}
There are two aspects of the pattern that causes complexity. First, the pattern complexity increases with the number of groups in the \RE. This kind of complexity will lead to better precision and lower coverage. Second, the pattern complexity also increases with the number of \emph{or}s (the symbol |) in a group. Here a group can be considered as a phrase set that expresses the same meaning. Therefore, unless adding ambiguous phrases to the group, this kind of complexity, generally results in higher coverage and slightly lower precision. 

Since, enriching a group is usually easier than designing the structure of the pattern (arranging the groups), we choose to modify the number of groups in a pattern to see the relationship between pattern complexity and performance.

Specifically, for intent detection, we simplify all the patterns with more than 2 meaningful groups (except for groups that match all the words), so that they will only have 2 meaningful groups. In total, 17 patterns are simplfied. The results are shown in Table \ref{simple_intent}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-F1 & Accuracy  \\
\hline
raw & 63.60 & 80.52 \\
\hline
raw+feat & 65.16 $\rightarrow$ TBA & 83.20 $\rightarrow$ TBA \\
\hline
raw+logit & 69.22 $\rightarrow$ 65.09 & 89.25 $\rightarrow$ 83.09  \\
\hline
two+both & 75.58 $\rightarrow$ 74.51 & 88.80 $\rightarrow$ 87.46 \\
\hline 
\end{tabular}
}
\caption{Intent Detection Results on 20-Shot Data with Simple Patterns}
\label{simple_intent}
\end{table}


As for slot filling, since the number of patterns with more than 2 meaningful groups is limited (only 7 patterns), we choose a more aggresive strategy that all the patterns are reduced to word list pattern, except for patterns whose major component is a number, which is not distinguishable at all on their own, or the word is a common word itself. Further, all the remaining patterns are ensured to have at most 2 meaningful groups. In total, 21 patterns are affected. See Table \ref{simple_slot}.

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
  & Macro-P/R/F1 & Micro-P/R/F1  \\
\hline
raw+feat & TBA & TBA  \\
\hline
raw+logit & TBA & TBA  \\
\hline 
\end{tabular}
}
\caption{Slot Filling Results on 20-Shot Data with Simple Patterns}
\label{simple_slot}
\end{table}




