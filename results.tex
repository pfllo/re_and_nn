\section{Experimental Results}
\label{sec:experiments}

\subsection{Few-Shot Learning}
\RE patterns are often used for classes with limited data in industry. Therefore, to answer question (1), we first explore the few-shot learning scenario.

\paragraph{Intent Detection}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy} \\
\hline
BLSTM & 45.28 / 60.02 & 60.62 / 64.61 & 63.60 / 80.52  \\
\hline
BLSTM+feat & 49.40 / 63.72 & 64.34 / 73.46 & 65.16 / 83.20   \\
\hline
BLSTM+logit & 46.01 / 58.68 & 63.51 / 77.83 & 69.22 / \textbf{89.25} \\
\hline
BLSTM+hu16 & 47.22 / 56.22 & 61.83 / 68.42 & 67.40 / 84.10  \\
\hline
two & 40.44 / 57.22 & 60.72 / 75.14 & 62.88 / 83.65  \\
\hline
two+posi & 50.90 / 74.47 & 68.69 / 84.66 & 72.43 / 85.78  \\
\hline
two+neg & 49.01 / 68.31 & 64.67 / 79.17 & 72.32 / 86.34   \\
\hline
two+both & \textbf{54.86} / \textbf{75.36} & \textbf{71.23} / \textbf{85.44} & \textbf{75.58} / 88.80   \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data}
\label{tab_intent_few}
\end{table}

As shown in Table \ref{tab_intent_few}, except for 5-shot, all the methods makes improvements to the baseline \texttt{BLSTM}, and using \RE to guide the attention module works best. Specifically, \texttt{posi} performs better than \texttt{neg} when the data is limited, which is because that the negative patterns are derived from the positive ones with some noise. However, \texttt{neg} works slightly better in the 20-shot setting, which is possibly due to that negative patterns significantly outnumbers the positive ones. Not surprisingly, we get the best results when positive and negative patterns are combined. Further, we can also see that \texttt{two} works better than \texttt{BLSTM} when we have enough data, indicating again the effectiveness of our two-side attention method.

Further, we can also see that \texttt{logit} works generally better than \texttt{feat}, except for the 5-shot case. This is probably because that adding a value directly to the logit of each intent is more clear and straightforward than using the average embedding of the intents predicted by \RE as feature. And the straightforwardness of \texttt{logit} also makes the prediction more sensitive to the noise of \RE, which leads to worse results in the 5-shot setting.

To compare with existing methods of combining \NN and rules, we also implement the teacher-student network \cite{liu2016attention}, which is a powerful method to combine first-order-logic (\FOL) constraints with \NN, and proves to perform well on various datasets. 
They first use \FOL rules to rectify the label distribution produced by \NN, and then let the \NN to learn from the rectified distribution, which a certain amount of enough data to support this process. 
Therefore, although \texttt{hu16} and \texttt{logit} both belongs to \emph{fusion with output},
since the \texttt{RE}-related parameters are limited in \texttt{logit},
\texttt{hu16} performs consistently worse than \texttt{logit} in this few-shot learning setting.
% \todo{try to acknowledge that \texttt{hu16} belongs to \emph{fusion in output}}

We can also see that, the improvements from \RE decreases as the size of training data increases, 
which is reasonable since as we have more data, the information contained in the data will have more overlap with the information in \RE. 

Besides, starting from 10-shot, \texttt{two+both} significantly outperforms pure \RE (see the \RE row in Table \ref{tab_full}), showing that our method do help generalizing the \RE patterns by fusing them with \NN. 


\paragraph{Slot Filling}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  &  \multicolumn{3}{|c|}{Macro-F1 / Micro-F1}   \\
\hline
BLSTM & 60.78 / 83.91 & 74.28  / 90.19 & 80.57 / 93.08 \\
\hline
BLSTM+feat & \textbf{66.84} / \textbf{88.96} & 79.67 / \textbf{93.64} & 84.95 / 95.0 \\
\hline
BLSTM+logit & 63.68 / 86.18 & 76.12 / 91.64  & 83.71 / 94.43 \\
\hline
BLSTM+hu16 & 63.37 / 85.37 & 75.67 / 91.06 & 80.85 / 93.47 \\
\hline
two & 59.4 / 83.29 & 73.22 / 90.08 & 79.58 / 92.57 \\
\hline
two+posi & 59.81 / 81.0 & 73.62 / 89.28 & 78.94 / 92.21 \\
% \hline
% two+posi+feat & 66.08 / 87.57 & \textbf{81.39} / 93.0 & 84.11 / 94.92\\
\hline
mem & 61.25 / 83.45 & 77.83 / 90.57 & 82.98 / 93.49 \\
\hline
mem+feat & 65.08 / 88.07 & \textbf{80.64} / 93.47 & \textbf{85.45} / \textbf{95.39} \\
\hline
\end{tabular}
}
\caption{Slot Filling Result on Few-Shot Data}
\label{tab_slot_few}
\end{table}

Different from intent detection, as shown in Table \ref{tab_slot_few}, the attention loss does not work for slot filling.
% \footnote{Negative pattern works even worse, we therefore only show the results of positive pattern here}. 
The reason is that, the tag of a word is decided mainly by the word itself, together with 0-3 words in the context to provide supplementary information.
% However, attention does not contribute to better self-awareness. 
Since \BLSTM output already models some context, and attention does not help understanding the word itself, the attention is not that useful as in intent detection, making the attention loss more of a burden than benifit.
Take the sentence in Table \ref{atis_sample} again for example, we recognize \emph{boston} as \emph{fromloc.city} because the word \emph{boston} reprensents a city, and it follows a context word \emph{from}. Since this simple context is easily captured by \BLSTM, attention does not help much in this case.
By examing the attention values of \texttt{two} trained on full dataset,
we find instead of marking informative context words, the attention tend to concentrate on the word that is to be predicted itself, which further confirms our hypothesis on attention loss. 
Note that, since negative attention performs even worse, we only show positive attention results here.

Another difference is that, although both \texttt{logit} and \texttt{feat} work, \texttt{feat} performs better this time. The reason is probably that, rather than simply adding a value to the logit of each label, \texttt{feat} introduces more information about the target word itself, which helps \NN make better understanding of the sentence. Further, with the same reason mentioned in the intent part, \texttt{logit} still performs consistently worse than \texttt{hu16}.

% It is also interesting that, \texttt{posi} and \texttt{feat} is complementary to each other. \texttt{posi+feat} performs better than pure \texttt{feat} itselft when we have enough data. This is probably because the extra light-weight NER from \texttt{feat} significantly reduces the need for self-awareness of the target word, which leaves possibility to better model the context (using attention).

Besides, we can see that even \texttt{BLSTM} outperforms \RE (see the \RE row in Table \ref{tab_full}) in 5-shot, showing that it is hard to write high-performance \RE patterns, but using \RE to boost \NN is feasible. 
% It is not surprising because this is not true 5-shot setting, extra data still exists for frequent patterns since one sentence may contain multiple slots.


\subsection{Compare with Memory Network}
To better understand the relationship between our method and existing few-shot learning methods, we also implement the memory network method \cite{kaiser2017learning}, which achieves good results in various few-shot settings. Specifically, by adapting their open-source code, we add their memory module to our \BLSTM model.
\todo{change title, do I need to mention \texttt{hu16} here?}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|}

\hline
  & \multicolumn{1}{|c|}{5-shot} & \multicolumn{1}{|c|}{10-shot} & \multicolumn{1}{|c|}{20-shot}  \\
 \hline
  & \multicolumn{3}{|c|}{Macro-F1 / Accuracy}   \\
\hline
BLSTM & 64.73 / 91.71 & 78.55 / 96.53 & 82.05 / 97.20 \\
\hline
BLSTM+hu16 & 65.22 / 91.94 & 84.49 / 96.75 & 84.80 / 97.42 \\
\hline
two & 65.59 / 91.04 & 77.92 / 95.52 & 81.01 / 96.86 \\
\hline
two+both & 66.62 / 92.05 & 85.75 / 96.98 & \textbf{87.97} / \textbf{97.76} \\
\hline
mem & 67.54 / 91.83 & 82.16 / 96.75 & 84.69 / 97.42 \\
\hline
mem+posi & \textbf{70.46} / \textbf{93.06} & \textbf{86.03} / \textbf{97.09} & 86.69 / 97.65 \\
\hline

\end{tabular}
}
\caption{Intent Detection Result on Few-Shot Data with the Top 3 Intents Having 300 Sentences.}
\label{tab_intent_few_fill}
\end{table}

To work properly, the memory module requires to be trained on either many few-shot classes or several classes with extra data.
Therefore, we expand our few-shot dataset for intent detection, so that the top 3 intents have 300 sentences. 
As shown in Table \ref{tab_intent_few_fill}, while the memory module works better than the base model, our attention loss also makes clear improvements in this setting. 
Further, the attention loss can also be combined with the memory module (\texttt{mem+posi}), and achieves better results than \texttt{mem} alone. 
Note that, since the attention module requires the input sentence to have only one embedding, we only use one set of possitive attention in this case.

As for slot filling, since we already have extra data for frequent tags in the original few-shot data, we use them directly to run the memory module. As shown in Table \ref{tab_slot_few}, \texttt{mem} also improves the base model, and it is also compatible to our \texttt{feat} fusion method, which gives \texttt{mem} a further boost.

Note that, for the compactness of the paper, we only combine the best-performing fusion method in each task with \texttt{mem}. Other methods in each task can be easily combined as well. 

% NOTE: since we have more data here, the importance of attention guidance is not that crucial as in the previous more strict few-shot leanring setting. Therefore, the weight of attention loss is reduced to 1 in this setting and the following full dataset setting, 


\subsection{Full Dataset}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|}

\hline
 & Intent & Slot \\ 
\hline
  & Macro-F1 / Accuracy &  Macro-F1 / Micro-F1 \\
\hline
BLSTM & 92.50 / 98.77  & 85.01 / 95.47\\
\hline
BLSTM+feat & 87.36 / 95.97 & 86.7 / 95.55\\
\hline
BLSTM+logit & 92.48 / 98.77 & 86.94 / 95.42  \\
\hline
BLSTM+hu16 & 93.09 / 98.77 & 85.74 / 95.33  \\
\hline
two & 93.64 / 98.88  & 84.45 / 95.05\\
% \hline
% two+posi & 93.02 & 98.88 \\
% \hline
% two+neg & 93.92 & \textbf{98.99} \\
\hline
two+both/posi & \textbf{96.20} / \textbf{98.99} & 85.37 / 95.34 \\
\hline
mem & 93.42 / 98.77 & 85.72 / 95.37\\
\hline
mem+posi/feat & 94.36 / \textbf{98.99} & \textbf{87.82} / \textbf{95.90} \\
\hline
\hline
RE & 70.31 / 68.98 & 42.33 / 70.79\\
\hline
L\&L16 & - / 98.43 & - / 95.98\\
\hline 

\end{tabular}
}
\caption{Results on Full Dataset. The left side of \texttt{two+both/posi} and \texttt{mem+posi/feat} applies for intent, and the right side applies for slot.} 
\label{tab_full}
\end{table}

To answer question (2), we also evaluate our methods on the full dataset. As seen in Table \ref{tab_full}, for intent detection, attention loss still works, \texttt{logit} achieves similar results to \texttt{BLSTM}, but \texttt{feat} performs significantly worse than the baseline. This shows that, although the informative words marked by \RE still helps, the low-performance \RE output no longer works when the labeled data is sufficient. While \NN can learn to assign low weights for \RE output in \texttt{logit}, the noise coming from the input feature is hard to eliminate, and therefore leading to bad results of \texttt{feat}.

As for slot filling, on the contrary, \texttt{feat} and \texttt{logit} still works as in the few-shot setting. 
The reason is probably that, 
the output of the \RE used for \texttt{feat} and \texttt{logit} provides extra information to help \NN better understand the target word (e.g., entity type), which is hard to inferred by \NN itself.
% The reason is probably that the output of the \RE used for \texttt{feat} and \texttt{logit} is only the entity part of the slot label, which is only indirectly connected to the prediction target. The noise contained in the indirectness makes \NN relies less on the \RE output, and therefore is less sensitive to the wrong predictions made by \RE as they do in intent detection.
Also note that, different from few-shot settings, \texttt{posi} improves \texttt{two} when we have enough data, and it also has better macro-F1 than \texttt{BLSTM}, showing that better attention can provide extra information when the key information are captured. 

With enough data, \texttt{hu16} performs better than \texttt{logit} in intent detection. 
However, since \texttt{hu16} is a general framework to combine \FOL rules, it is more indirect in transfering knowledge from rules to \NN than our methods. Therefore, its performance is still clearly inferior to attention loss in intent detection and \texttt{feat} in slot filling, which are designed specifically for \RE rules.
% The reason that it still performs worse than attention loss intent detection and in slot filling, is 

Further, \texttt{mem} still generally works, and we can also make improvements by further combining our fusion methods. Besides, the state-of-art results on the ATIS data produced by \cite{liu2016attention} is also included (\LL). We can see that our base \BLSTM model achieves comparative results to \LL, confirming that the improvements from our fusion method does not come from the inferior ability of the base model\footnote{Since slot filling is evaluated in phrase level, there is almost no difference in F1 when we convert the prediction on our split data format to the original format (see Section \ref{sec_datasest}), making our results still comparatable to \LL.}. And the analysis on different settings in the sections above together answer question (3).

\subsection{Complexity of the Pattern}

\begin{table}
\setlength{\tabcolsep}{0.23em}
\centering
\small{
\begin{tabular}{|c|c|c|c|c|}

\hline
  & \multicolumn{2}{|c|}{Intent} & \multicolumn{2}{|c|}{Slot}  \\
\hline
  & \multicolumn{2}{|c|}{Macro-F1 / Accuracy} & \multicolumn{2}{|c|}{Macro-F1 / Micro-F1}  \\
\hline
  & Before & After & Before & After \\
\hline
base & \multicolumn{2}{|c|}{63.60 / 80.52} & \multicolumn{2}{|c|}{80.57 / 93.08}  \\
\hline
+feat & 65.16/\textbf{83.20} & \textbf{66.51}/80.40 & \textbf{84.95/95.00} & 83.88/94.71 \\
\hline
+logit & \textbf{69.22/89.25} & 65.09/83.09 & \textbf{83.71/94.43} & 83.22/93.94  \\
\hline
+both & \textbf{75.58/88.80} & 74.51/87.46 & - & - \\
\hline 
\end{tabular}
}
\caption{Results on 20-Shot Data with Simple Patterns. \texttt{base} is \texttt{BLSTM}, the base model for \texttt{+feat} and \texttt{+logit}. \texttt{+both} refers to \texttt{two+both}.}
\label{tab_simple}
\end{table}

Since enriching the number of phrases in a group with $|$ is usually easier adding new groups, we choose to modify the number of groups in a pattern to see the relationship between pattern complexity and fusion performance.

Specifically, for intent detection, we simplify all the patterns with more than 2 groups (groups matching a sequence of any words are excluded), so that only 2 groups are kept, which influences 17 patterns. 
As for slot filling, since the number of patterns with more than 2 groups is limited (only 7 patterns), we choose a more aggresive strategy that all the patterns are reduced to word list pattern, except for those trying to tag a a group that matches a number or a common word, which is not distinguishable at all on its own. 
All the remaining patterns are ensured to have at most 2 groups, and 21 patterns are affected in total.

As shown in Table \ref{tab_simple}, simplifying the \RE generally results in worse performance, but the results are still better than the base \BLSTM. 
This shows that, while the quality of the \RE matters when fused with \NN, we can still get useful information from low-quality patterns. Further, we can see that attention loss is less sensitive to the simplification than other methods. 
The reason probably lies in that, although simplified, the core informative words still remains in the pattern, which is therefore still helpful for the attention module\footnote{
We do not include simplification results of attention loss for slot filling because its \texttt{RE}s are different from the other two methods, and we already know it does not work for slot filling}.






