\section{Conclusions}
In this paper, we investigate different ways for combining \NNs and \REs for solving typical \SLU tasks. Our experiments demonstrate that
combining \NNs and \REs clearly improves the \NN performance in both the few-shot learning and the  full dataset settings. We show that by
exploiting the implicit knowledge encoded within \REs, one can significantly improve the learning performance. Specifically, we observe
that using \REs to guide the attention module works best for intent detection, and using \REtags as features is an effective approach for
slot filling. We provide interesting insights on how \REs of various forms can be employed to improve \NNs, showing that while complex \REs
tend to yield better results, simple \REs are also useful.
