\section{Approach}

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=0.85\textwidth]{figure/re_nn_overview.png}    
\caption{Overview of our methods. The left side shows the methods for intent detection, and the right side shows an example for predicting the slot lable for the third word. \circled{1}, \circled{2}, \circled{3} refers to the methods in Section \ref{fusion_with_input}, \ref{interact_with_module}, \ref{fusion_with_output} respectively, and the \RE that applies to the input sentence is shown in the bottom.}
\label{fig_overview}
\end{center}
\vspace{-1em}
\end{figure*}

As shown in Fig. \ref{fig_overview}, we will discuss 3 kinds of methods of combining \NN and \RE for both intent detection and slot filling.

\subsection{Base Models}
We use Bi-directional LSTM (\BLSTM) as our base \NN model for both intent classification and slot filling, since it proves to perform well in both of these two tasks \cite{liu2016attention}. 
% Further, to obtain sentence embedding for intent detection, we use a self-attention layer upon the \BLSTM output.
\paragraph{Intent Detection}
As shown in Fig. \ref{fig_overview}, given the word embeddings $[\textbf{x}_1, ..., \textbf{x}_n]$ as input, \BLSTM outputs $[\textbf{h}_1, ..., \textbf{h}_n]$ for each word, where $n$ is the sentence length. To obtain sentence embedding for further intent prediction, we also add a self attention layer upon the \BLSTM output, and the attention $\alpha_i$ for each word $i$ is:
\begin{equation}
\alpha_i=\frac{exp(\textbf{h}_i\textbf{Wc})}{\sum_{i}{exp(\textbf{h}_i\textbf{Wc})}}
\end{equation}
where $\textbf{c}$ is a randomly initialized context vector used to select important words for classification, and $\textbf{W}$ is a weight matrix. The final sentence embedding is abtained by: 
\begin{equation}
\textbf{s} = \sum_{i}{\alpha_i\textbf{h}_i}
\end{equation}
and $\textbf{s}$ is fed to a softmax classifier for classfication.

\paragraph{Slot Filling}
The model for the slot filling task is more straightforward. Given the output of the Bi-LSTM $[\textbf{h}_1, ..., \textbf{h}_n]$, the tag prediction of the word $i$ is generated directly by a softmax classifier which takes $\textbf{h}_i$ as input.\footnote{The attention aggregation in Fig. \ref{fig_overview} is only used for the mothod in Section \ref{interact_with_module}.}


\subsection{Fusion with Input}
\label{fusion_with_input}
Similar to the stacking technique \cite{wolpert1992stacked}, a straightforward way to combine \RE and \NN is to use the output of \RE patterns as feature, and feed them as the input of \NN models.
\paragraph{Intent Detection}
The output of \RE patterns for intent detection is a tag for the entire sentence, which may or may not be the same as intent labels.\footnote{If no \RE matches, we will assign a special tag indicating no matches. This process is applied to all the settings.} 
And these tags may possibly conflict with each other due to the imperfection of the patterns. 
For example, the sentence \emph{list the delta airlines flights from boston to philadelphia} may match a pattern \texttt{/list(\;the)?\;\_\_AIRLINE/} that output tag \emph{airline}, 
and match a pattern \texttt{/list(\;\textbackslash w+)\{0,3\}?\;flights?/} that output tag \emph{flight} as well.
% \footnote{\texttt{\_\_WORD} matches a single word, which can be \texttt{/\textbackslash w+/}.}

% Many paper has proved that, embed the discrete feature is an effective method to combine features in \NN \cite{zeng2014relation, guo2017deepfm}. 
Since there may exist several tags for each sentence, we will average the tag embeddings to form an aggregated embedding as input. 
Then, there are two ways to add this embedding: (1) append it to the embedding of every input word, (2) append it to the input of the softmax classifier (see \circled{1} in the left part of Fig. \ref{fig_overview}). Note that, in the first method, the tag embedding is actually copied $n$ times, where $n$ is the sentence length. In our pilot experiments, we find that the copied tag embedding tend to dominate the input, which makes \NN heavily relies on \RE output, and therefore makes the overall performance similar to the performance of \RE alone. Consequently, we will adopt the second method in this paper.

\paragraph{Slot Filling}
Since the output of slot filling \RE patterns will output word-level tags, as shown in \circled{1} in the right part of Fig. \ref{fig_overview}, we can simply embed the output tags and append it to input word embeddings. 
Further, since our slot label is subject to the BIO annotation paradigm, we further extend the output tag of \RE to BIO format. For example, the final \RE tag of phrase \emph{new york} is \emph{B-city I-city} if its original tag is \emph{city}.

\subsection{Interact with NN Module}
\label{interact_with_module}
Since the \RE pattern itself has already highlighted informative words for the output tag, it can also help us guide the attention module in \NN.
As shown in \circled{2} in Fig. \ref{fig_overview}, the bold blue arrows and words indicates the informative words marked by \RE for this prediction.
\paragraph{Intent Detection}
Taking the sentence in Table \ref{atis_sample} again for example, the pattern \texttt{/\textasciicircum flights?\:from/} that leads to tag \emph{flight} means that, we should attend to \emph{flights from} in the input sentence to output the intent \emph{flight}.

Different from the base intent model, we make two changes to better incorporate guidance from \RE.
First, since different intent has their specific informative words, using a single sentence embedding, which is produced by only one set of attention, would make the attention less focused. 
Considering we also know the intent that each \RE points to, we therefore let each intent $i$ use diffenrent attentions $\textbf{a}_i$, which is used to generate the sentence embedding $\textbf{s}_i$ for that intent:
\begin{equation}
\alpha_{ij}=\frac{exp(\textbf{h}_j\textbf{W}_a\textbf{c}_i)}{\sum_{j}{exp(\textbf{h}_j\textbf{W}_a\textbf{c}_i)}}, \quad
\textbf{s}_i = \sum_{j}{\alpha_{ij}\textbf{h}_j}
\label{label_att_eq}
\end{equation}
where $\textbf{c}_i$ is the context vector for intent $i$ which is used to compute attention, $\textbf{h}_j$ is the \BLSTM output for word $j$, and $\textbf{W}_a$ is a weight matrix.

The the probability $p_i$ that the input sentence belongs to intent $i$ is computed by:
\begin{equation}
logit_i=\textbf{w}_i\textbf{s}_i + b_i, \quad\quad p_i = \frac{logit_i}{\sum_{i}{logit_i}}
\label{label_prob_eq}
\end{equation}
where $\textbf{w}_i$, $logit_i$, $b_i$ are weight vector, logit, and bias for intent $i$ respectively.

Second, apart from indicating a sentence belongs to intent $i$ (\emph{positive patterns}), an \RE pattern can also indicate a sentence should not be labeled as intent $i$ (\emph{negative patterns}). Therefore, to make use negative patterns, we use another set of attention to compute a new set of logits for each intent using Equation \ref{label_att_eq} and \ref{label_prob_eq}. By denoting the logits computed by positive patterns as $logit_{pi}$, and the ones computed by negative patterns as $logit_{ni}$, the final logit for intent $i$ is then: 
\begin{equation}
logit_i = logit_{pi} - logit_{ni}
\end{equation}

To use \RE patterns to guide the attention, we add an attention loss to the final loss:
\begin{equation}
loss_{att} = \sum_{i}{m_i\sum_{j}{t_{ij}log(\alpha_{ij})}}
\label{att_loss}
\end{equation}
where $t_{ij} = 0$ when none of the matched patterns that lead to intent $i$ mark word $j$ as informative (\emph{clue word}), otherwise $t_{ij} = 1/k_{i}$ where $k_i$ is the number of clue words mark by \RE for intent $i$. $m_i$ is a 0-1 indicator that equals 1 only when there is a matched \RE pattern that leads to intent $i$. We use Equation \ref{att_loss} to compute both the attention loss $loss_{att\_p}$ for positive patterns and $loss_{att\_n}$ for negative patterns, and the final loss is:
\begin{equation}
loss = loss_{c} + \beta_p loss_{att\_p} + \beta_n loss_{att\_n}
\end{equation} 
where $loss_{c}$ is the original classification loss, $\beta_p$ and $\beta_n$ are weights for the two attention loss.

In practice, while with some noise, one positive pattern for intent $i$ can often be negative patterns for other intents. For example, the pattern \texttt{/\textasciicircum flights?\:from/} that indicates tag \emph{flight} also indicates that this sentence should not be labeled as tags like \emph{meal}, \emph{quantity}, and etc.
Therefore, we simply use the positive pattern for intent $i$ as the negative patterns for other intents in our experiments.

\paragraph{Slot Filling}
While we can apply the same \emph{two-side attention} (positive and negative attention) method as we do in intent prediction, we face an efficiency problem in slot filling. Since we need to assign a label for each word, if we still compute attention for each tag, we will have to compute $2\times L \times n$ attention values, where $L$ is the number of tags and $n$ is the sentence length. Considering the BIO tagging format further doubles the number of tags, this will result in a lot of computation and memory storage. 

Therefore, we use the simplified version of two-side attention. Specifically, all the tags share the same set of positive and negative attention, and therefore we have only one sentence embedding $\textbf{s}_{pi}$ generated from positive attention, and another embedding $\textbf{s}_{ni}$ from negative attention for each word $i$. The final prediction $\textbf{p}_i$ for word $i$ is then:
\begin{equation}
\begin{split}
\textbf{p}_i = softmax((\textbf{W}_p [\textbf{s}_{pi}; \textbf{h}_i] + \textbf{b}_p) \\- (\textbf{W}_n [\textbf{s}_{ni}; \textbf{h}_i] + \textbf{b}_n))
\end{split} 
\end{equation} 
where $\textbf{W}_{p}$, $\textbf{W}_{n}$, $\textbf{b}_{p}$, $\textbf{b}_{n}$ are weight matrices and bias vecotrs for positive and negative attention respectively. Here we add the \BLSTM output $\textbf{h}_i$ because the word $i$ itself also plays an important part in identifying its label.

\subsection{Fusion with Output}
\label{fusion_with_output}
The final type of combination is to use \RE to modify the output of \NN, and the method is the same for the two tasks (see \circled{3} in Fig. \ref{fig_overview}). 

% Therefore, instead using a teach-student framework, this enables us to directly influence the logits of each label in a trainable way, so that we do not need to assign a weight for each pattern, and significantly reduces the number of hyper-parameters.

As mentioned in Section \ref{re_desc}, the intent \RE patterns output the intent label directly, while the slot \RE patterns used by this fusion method only output the entity part of the slot label. To make connections between the \RE tag and the slot label, we further annotate all the slot labels that the output tag may lead to. For example, patterns that output tag \emph{city} will lead to three slot labels: \emph{fromloc.city}, \emph{toloc.city}, \emph{stoploc.city}.
% Actually, annotating this kind of connections is not difficult, since the \RE tags are generally somewhat related to the target label that we are trying to predict, otherwise the \RE tags will not be able to provide useful information for the prediction.

Let $z_i$ be a 0-1 indicator of whether at least one \RE pattern that leads to label $i$ matches the input sentence, then the final logits of label $i$ for the sentence (or a specific word when doing slot filling) is computed as: 
\begin{equation}
logit_i = logit'_i + w_i z_i
\end{equation}
where $logit'_i$ is the logit produced by the original \NN, and $w_i$ is a trainable weight indicating the overall confidence for patterns that lead to label $i$. Here we do not assign a trainable weight for each pattern, because there is a good chance that there is only a limited number of sentences matching a specific \RE. 
% Further, if we have strong prior confidence on the patterns, we can also easily incorporate these human-assigned pattern weights by simply multiplying it to $w_i$.


