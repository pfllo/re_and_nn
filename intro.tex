\section{Introduction}


%One of the fundamental techniques in computer science is the
Regular expressions (\REs) are widely used in various natural language processing (\NLP) tasks like pattern matching, sentence
classification, sequence labeling, etc.~\cite{chang2014tokensregex}.
%In dialogue systems, a \RE
%pattern \texttt{/\textasciicircum flights? from/} can help recognize the sentence in Fig.~\ref{atis_sample} has an intention of
%\emph{flights}, indicating the user is looking for flight-related information.
As a technique based on human-crafted rules, it is concise, explainable, tunable, and does not rely on much training data to generate. As
such, it is commonly used in industry, especially when available training examples are limited -- a problem known as few-shot
learning~\cite{gc2015big}.
% a problem known as few-shot learning.

While powerful, \REs have a poor generalization ability because all synonyms and variations in a \RE must be explicitly specified. As a
result, they often have a low coverage of \z{low coverage of what??}\lb{change coverage to recall?}. To overcome this limitation, \REs are often used together with
data-driven methods like neural network (\NN) based techniques, where a set of carefully-written \REs are used to handle certain cases with
high precision, leaving the rest for a data-driven method.

However, the use of \REs can go beyond simple pattern matching.
% and existing approaches only scratch the surface of what could possibly be achieved.
Our key insight is that a \RE often encodes a developer's knowledge of the problem domain, which indicates e.g., the informative words
(\textbf{\textit{clue
words}}) in its surface form. %pattern.
 % by including informative words (i.e., \textbf{\textit{clue words}}) of certain patterns.
% We argue that such information can help models to extract the implicit knowledge from the training data,
% which in turn enables data-driven methods to work effectively in the few-shot learning scenario with a small volume of annotated data.
We argue that such information can be utilized by data-driven methods to achieve better prediction results, especially in the few-shot
learning scenario.




% One of the major hurdles of combining \REs with data-driven methods is that \REs are often of different qualities -- while high-quality
% \REs are precise in capturing patterns, they are hard to generate, and although low quality \REs are easy to write, they can be noisy. This
% work seeks to overcome this hurdle by employing \NNs to control the noise. We choose \NNs because they are widely used in many \NLP
% tasks~\cite{goldberg2017neural}, are proven to be effective in tolerating noises~\cite{xie2016disturblabel}, and can improve the
% generalization ability of \REs. The result is a new way of using \REs,  \cyan{\sout{with a generic framework for quality control}}.

This work investigates the use of \REs to improve \texttt{NNs} -- a successful learning framework in many \NLP
tasks~\cite{goldberg2017neural}. This unique combination also enables the resulting framework to exploit \NNs' noise tolerance
capability~\cite{xie2016disturblabel}. This feature allows one to utilize a diverse set of \REs (Sec.\ref{re_desc}), including those
carefully-tuned complex pattern (which are high precision but are time-consuming to generate) and those simple ones (that are easy to
write, but can be noisy and less accurate).

%\red{By doing so,
%we are able to utilize both high-quality \REs (which are precise in capturing patterns but are time-consuming to generate) and those of low quality (that are easy to write, but can be noisy), % low-precision \REs,
%since \NNs are known to be good at tolerating noises~\cite{xie2016disturblabel}.}
% This may help reduce the difficulty of creating \REs, since high-precision \REs
% are usually more complex %than low-precision ones, and therefore are
% hard to generate.

% On the other side, since \NN models rely on distributed representation, they can generalize the \RE pattern so that phrases with similar meaning can also been matched, which may possibly result in better performance than using the \RE alone.

% On the other hand, instead of encoding knowledge into massive labeled data, human tend to express their knowledge in a more compact way, i.e., the rules. The rules accumulated by domain experts makes it an excellent source to compensate the shortage of labeled data.


% We propose a novel approach to combine \REs with a \NN at different levels.
This paper proposes novel approaches to combine \REs with \NNs at different levels.  At the input layer, we propose to use the evaluation
outcome of \REs as the input features of a \NN (Sec.\ref{fusion_with_input}).
%This shares a similar spirit with the stacking technique~\cite{wolpert1992stacked} for short text classification~\cite{wang2017combining}.
At the network module level, we show how to
exploit the knowledge encoded in \REs to guide the attention mechanism of a \NN (Sec.~\ref{interact_with_module}). At the output layer, we
combine the evaluation outcome of a \RE with the \NN output in a learnable manner (Sec.~\ref{fusion_with_output}).
% This allows the learning framework to exploit a diverse set of \REs while controlling the noises brought by low-quality \REs.

%(2) On the \NN module side, since \REs explicitly highlight clue words for a specific tag, we can use \RE to guide the attention mechanism
%in \NN. (3) On the \NN output side, we can combine the output of \RE and \NN in
%a direct but learnable way, so that the final output benefits from both directly. % \NN and \RE.
%Hu et al.~\shortcite{hu2016harnessing} also explored this aspect by using first-order-logic (\FOL) constraints to
%affect the \NN output in a teacher-student network manner. Different from their general framework, since \RE output is
%usually related to the label that we are trying to predict, we use a more direct way for combination.
% However, since it is a general framework, it sacrifices some directness of the combination for its generalityã€‚
% but they are more focused on constraints that should not be broke rather than the positive signals produced by \RE.
%
We evaluate our approach by applying it to two spoken language understanding (\SLU) tasks, namely \emph{intent detection} and \emph{slot
filling}, which respectively correspond to two fundamental \NLP tasks: sentence classification and sequence labeling. To demonstrate the
usefulness of \REs in scenarios where the available annotated data vary, we explore both the few-shot learning setting and the one with
full training data. Experimental results show that our approach is highly effectively at exploiting the available training data, yielding
significant better learning performance over the \RE-unaware method.


%To guide \RE annotation,
%We also examine the impact of \RE complexity to the \NN performance.
% The main contribution of this work is a novel learning framework that combines \REs and \NNs to effectively exploit the available training
% data. We show that our framework can successfully utilize the human knowledge encoded in \REs while control the inevitable noises
% introduced by \REs of various qualities and improve the generalization ability of \REs.

%Experimental results show that our approach can significantly improve the learning effectiveness of \NNs in both the few-shot learning and
%the full annotation settings.

Our contributions are as follows. (1) We present the first work to systematically investigate methods of combining \REs with \NNs. (2) The
proposed methods are shown to clearly improve the \NN performance in both the few-shot learning and the full annotation settings. (3) We
provide a set of guidance on how to combine \REs with \NNs and \RE annotation.
