\section{Introduction}


One of the fundamental techniques in computer science is the regular expression (\RE), a language for specifying text search strings.  \RE is used for a wide range of natural language processing (\NLP) tasks like pattern matching, sentence classification, sequence labeling etc.~\cite{chang2014tokensregex}. 
For example, in dialogue system, the \RE pattern \texttt{/\textasciicircum flights? from/} can help
recognizing the sentence in Table~\ref{atis_sample} as expressing intent \emph{flights}, which indicates that the user is looking for
flight-related information. As a technique based on human-generated rules, it is explainable, tunable, and does not rely on training data,
making it widely used in industry and especially in scenarios where the training data is limited (few-shot learning)~\cite{gc2015big}.

On the other hand, since all the synonyms and variations need to be explicitly specified, the generalization ability of \RE is rather poor,
which leads to low coverage of the \REs. Therefore, in practice, \RE is often combined with data-driven methods like neural network
(\NN), where \RE is considered as an easy-to-tune method to deal with a certain fraction of cases with high-precision.

However, the value of \RE is more than that. When writing an \texttt{RE}, people have encoded their knowledge about the problem in it, which can be used to improve data-driven models like \NN. For example, the \RE itself has actually indicated the informative words
(\textbf{\emph{clue words}}) for its prediction.

Since \NN proves to perform generally well in vairous \NLP tasks~\cite{goldberg2017neural}, in this paper,
we investigate the methods of using \RE to improve \NN. By doing this, different from the scenario where we only use high-precision
\REs to handle a fraction of cases, we can further make use of low-precision \REs, since \NN is known to be good at
tolerating noises~\cite{xie2016disturblabel}. This also reduces the difficulty of writing \REs, since high-precision \REs
are usually more complex than low-precision ones, and therefore are hard to generate.

% On the other side, since \NN models rely on distributed representation, they can generalize the \RE pattern so that phrases with similar meaning can also been matched, which may possibly result in better performance than using the \RE alone.

% On the other hand, instead of encoding knowledge into massive labeled data, human tend to express their knowledge in a more compact way, i.e., the rules. The rules accumulated by domain experts makes it an excellent source to compensate the shortage of labeled data.


Specifically, we explore fusing \RE with \NN in three different aspects: (1) On the \NN input side, we can use the \RE output as features
to \NN. This shares similar spirit with the stacking technique~\cite{wolpert1992stacked} and has also been used by~\cite{wang2017combining}
to incorporate knowledge base rules in short text classification.
(2) On the \NN module side, since \REs highlight clue words for a
specific tag, we can use \RE to guide the attention module in \NN. (3) On the \NN output side, we can combine the output of \RE and \NN in
a learnable way, so that the final output contains information from both \NN and \RE. Hu et al.~\shortcite{hu2016harnessing} also explored
this aspect by using first-order-logic (\FOL) constraints to affect the \NN output in a teacher-student network manner. Different from
their general framework, since \RE output is usually related to the label that we are trying to predict, we use a more direct way for
combination.
% However, since it is a general framework, it sacrifices some directness of the combination for its generalityã€‚
% but they are more focused on constraints that should not be broke rather than the positive signals produced by \RE.

We experiment our methods in the spoken language understanding (\SLU) scenario. We choose this task because it contains intent detection
and slot filling as two subtasks, which correspond to two of the most important tasks in \NLP: sentence classification, sequence labeling.
Further, this is also a task where \RE is heavily used in industry.

We explore both the few-shot learning setting where the data is limited and the setting with the full dataset, to see how \RE helps when we have different amounts of data.
To guide \RE annotation, we also conduct analysis on the impact of \RE complexity to the performance \NN.

Our contributions are as follows. (1) This is the first work to systematically investigate methods for fusing \RE with \NN. (2) The
extensive experiments show that our methods clearly improve the \NN performance in both few-shot learning settings, and the settings with
full dataset. (3) Our analysis provides meaningful guidance to fusing method selection, and \RE annotation as well.
\todo{Luo: unify the terminology of \emph{fuse} and \emph{combine}, which to choose?}
