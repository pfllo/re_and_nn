
\section{Introduction}


%One of the fundamental techniques in computer science is the
Regular expressions (\REs) are widely used in various natural language processing (\NLP) tasks like pattern matching, sentence
classification, sequence labeling, etc.~\cite{chang2014tokensregex}.
%In dialogue systems, a \RE
%pattern \texttt{/\textasciicircum flights? from/} can help recognize the sentence in Fig.~\ref{atis_sample} has an intention of
%\emph{flights}, indicating the user is looking for flight-related information.
As a technique based on human-crafted rules, it is concise, interpretable, tunable, and does not rely on much training data to generate. As
such, it is commonly used in industry, especially when the available training examples are limited -- a problem known as few-shot
learning~\cite{gc2015big}.
% a problem known as few-shot learning.

While powerful, \REs have a poor generalization ability because all synonyms and variations in a \RE must be explicitly specified. As a
result, %they often have a low coverage of \z{low coverage of what??}\lb{change coverage to recall?}. To overcome this limitation,
\REs are often ensembled  with data-driven methods, such as neural network (\NN) based techniques, where a set of carefully-written \REs
are used to handle certain cases with high precision, leaving the rest for data-driven methods.

We believe the use of \REs can go beyond simple pattern matching. In addition to being a separate classifier to be ensembled, a \RE also
encodes a developer's knowledge for the problem domain. The knowledge could be, for example,  the informative words (\textbf{\textit{clue
words}}) within a \RE's surface form. We argue that such information can be utilized by data-driven methods to achieve better prediction
results, especially in few-shot learning.



This work investigates the use of \REs to improve \texttt{NNs} -- a learning framework that is widely used in many \NLP
tasks~\cite{goldberg2017neural}. The combination of \REs and a \NN  allows us to exploit the conciseness and effectiveness of \REs and the
strong generalization ability of \NNs. This also provides us  an opportunity to learn from various kinds of  \REs, since \NNs are known to
be good at tolerating noises~\cite{xie2016disturblabel}.


This paper presents novel approaches to combine \REs with a \NN at different levels.  At the input layer, we propose to use the evaluation
outcome of \REs as the input features of a \NN (Sec.\ref{fusion_with_input}). At the network module level, we show how to exploit the
knowledge encoded in \REs to guide the attention mechanism of a \NN (Sec.~\ref{interact_with_module}). At the output layer, we combine the
evaluation outcome of a \RE with the \NN output in a learnable manner (Sec.~\ref{fusion_with_output}).
% This allows the learning framework to exploit a diverse set of \REs while controlling the noises brought by low-quality \REs.

%(2) On the \NN module side, since \REs explicitly highlight clue words for a specific tag, we can use \RE to guide the attention mechanism
%in \NN. (3) On the \NN output side, we can combine the output of \RE and \NN in
%a direct but learnable way, so that the final output benefits from both directly. % \NN and \RE.
%Hu et al.~\shortcite{hu2016harnessing} also explored this aspect by using first-order-logic (\FOL) constraints to
%affect the \NN output in a teacher-student network manner. Different from their general framework, since \RE output is
%usually related to the label that we are trying to predict, we use a more direct way for combination.
% However, since it is a general framework, it sacrifices some directness of the combination for its generalityã€‚
% but they are more focused on constraints that should not be broke rather than the positive signals produced by \RE.
%
We evaluate our approach by applying it to two spoken language understanding (\SLU) tasks, namely \emph{intent detection} and \emph{slot
filling}, which respectively correspond to two fundamental \NLP tasks: sentence classification and sequence labeling. To demonstrate the
usefulness of \REs in real-world scenarios where the available number of annotated data can vary, we explore both the few-shot learning
setting and the one with full training data. Experimental results show that our approach is highly effective in utilizing the available
annotated data, yielding significantly better learning performance over the \RE-unaware method.


%To guide \RE annotation,
%We also examine the impact of \RE complexity to the \NN performance.
% The main contribution of this work is a novel learning framework that combines \REs and \NNs to effectively exploit the available training
% data. We show that our framework can successfully utilize the human knowledge encoded in \REs while control the inevitable noises
% introduced by \REs of various qualities and improve the generalization ability of \REs.

%Experimental results show that our approach can significantly improve the learning effectiveness of \NNs in both the few-shot learning and
%the full annotation settings.

Our contributions are as follows. (1) We present the first work to systematically investigate methods for combining \REs with \NNs. (2) The
proposed methods are shown to clearly improve the \NN performance in both the few-shot learning and the full annotation settings. (3) We
provide a set of guidance on how to combine \REs with \NNs and \RE annotation.
