\section{Evaluation Methodology}
Our experiments aim to answer three main questions: (1) Does \RE patterns help when training data is limited? (2) Does \RE patterns help when using the full training data? (3) When do the three combination methods work, and which works best in each scenario?

\subsection{Datasets}
ATIS (Airline Travel Information Systems) dataset \cite{hemphill1990atis} is a widely used benchmark in \NLU research. It involves queries about flights, meal, abbreviations and etc. We follow the setup used by \cite{liu2016attention}, with 4978 queries for training and 893 for test. There are 127 slot labels and 18 intents. Numbers are replaced with special tokens like \emph{DIGIT*m}, where \emph{m} is the number of digits in the original string. Different from previous work, we also separate words like \emph{Miami\'s} into \emph{Miami \'s} to reduce the number of unseen words, which is important to few-shot leanring.

To answer question (1), we also explore the few-shot learning scenario. Specifically, for intent detection, we randomly select 5, 10, 20 instances from the training set for each intent to form the few-shot learning training set. As for slot filling, we first sort the slot labels by frequence, and randomly select sentences for the slot labels in the order from the least-frequent label to the most-frequent one. We also explore 5, 10, 20 shots settings. Note that since one sentence typically involves multiple slot labels, the number of frequent slot labels may exceed target number of shots.
Further, to make the \RE reusable across different settings, $k_1$-shot dataset is contained by $k_2$-shot dataset if $k_1 < k_2$. 
We use the original test set for both intent detection and slot filling.
% After that, we move to the adjacent label which is slightly more frequent, and make sure the number of training instances meet the threshold.

Since most few-shot learning methods require either extra classes or classes with enough data for training, we also explore a new setting for intent detection to make fair comparison with existing few-shot learning methods. Specifically, we let the 3 most frequent intents to have 300 training instances, and the rest of the few-shot learning dataset remains untouched, which is also a common scenario in real world, where we often have several frequent classes and many classes with limited data.

\subsection{RE Patterns}
\label{re_in_exp}
We only use the 20-shot learning dataset to produce \RE patterns, but word lists like city lists are collected using the full dataset. 
While the word list can be generated automatically, the major complexity of annotation comes from the number of groups in \RE.
Typically, annotating intent patterns are simple, which takes about 1-2 minutes to yield one pattern, and 54 patterns are collected in total, with averagely 2.2 groups for each \RE (the group matching a sequence of any words is not counted). 
Similarly, annotating slot filling patterns to provide feature or modify output is also easy since we only annotate the entity part of the slot label. It also takes about 1-2 minutes to yield one pattern, and 60 patterns are finnally produced, with averagely 1.7 groups.
However, annotating slot filling patterns to guide attention is difficult, since we need to carefully select informative words, and we also need to target for the full slot label as well. Typically, we need about 2-5 minutes to generate one pattern, and we obtained 115 patterns in total, with averagely 3.3 groups.
% These patterns a generated in a single run, and not modified during experiments.


\subsection{Experimental Setup}
\paragraph{Hyper-parameters}
We use similar hyper-parameters to \cite{liu2016attention} and achieves comparative results in the original dataset. Specifically, we have batch size 16, dropout probability 0.5, Bi-LSTM size 200 (100 for each direction), attention loss weight 16 (both positive and negative) for few-shot, and weight 1 when we have more data (see Section \ref{sec:experiments} for details). We use 100-dimensional GloVe \cite{pennington2014glove} word vector trained on Wikipedia and Gigaword, and Adam optimizer \cite{kingma2014adam} with learning rate 0.001.

\paragraph{Evaluation Metric}
For intent detection, we report the overall accuracy and macro-F1. And we report micro-F1 and macro-F1 for slot filling.
The micro/macro-F1 are the harmonic mean of the micro/macro precision and recall.
The macro-precision/recall are calculated by averaging the precision and recall of each label, and the micro-precision/recall are averaged over each prediction. 
The accuracy and micro statistics shows the overall performance for all the instances, while the macro statitics is more sensitive to low-performance classes even when their test data is limited.

\paragraph{Naming Conventions}
\texttt{BLSTM} refers to attentional \BLSTM for intent detection and vanila \BLSTM for slot filling, \texttt{two} refers to Bi-LSTM with two-side attention (but without attention guidance), \texttt{+feat} means adding \RE output as feature, \texttt{+posi} and \texttt{+neg} refers to using positive and negative attention loss respectively, \texttt{+both} refers to use both of the attention loss, \texttt{RE} refers to using the \RE pattern output as prediction directly\footnote{for slot filling, patterns for attention guidance are used}, \texttt{mem} means combining \texttt{BLSTM} and a memory module which performs well on few-shot learning \cite{kaiser2017learning}, \LL means the joint model \cite{liu2016attention} which achives state-of-art results in ATIS. 



