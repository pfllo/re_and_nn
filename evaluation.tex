\section{Evaluation Methodology}
Our experiments aim to answer three questions: (1) Do \RE patterns help when data is limited? (2) Do \RE patterns help when using the full training data? (3) When do the three combination methods work, and which works best in each scenario?
% \todo{how to refer to question (3) in exp?}

\subsection{Datasets}
\label{sec_datasest} ATIS (Airline Travel Information Systems) dataset \cite{hemphill1990atis} is widely used in \SLU research. It involves
queries about flights, meal and etc. We follow the setup of \cite{liu2016attention}, with 4978 queries for training and 893 for test. There
are 18 intents and 127 slot labels. Numbers are replaced with special tokens like \textsl{\underline{DIGIT*m}}, where \emph{m} is the
number of digits in the original string. Different from previous work, we also split words like \textsl{\underline{Miami's}} into
\textsl{\underline{Miami 's}} \FIXME{ZW: Why we want to put a space in betwen Miami and 's to reduce the \#unseen words?} to reduce the
number of words \orange{that does not have pretrained word embeddings, which is important to few-shot learning}.
\orange{LUO to ZW: Due to limited data, few-shot learning heavily relies on pretrained word embeddings to understand words. Inserting the space increases the chance that the word can be found in pretrained word embeddings, and therefore significantly improves the performance of \NN on few-shot learning. This process makes sure that our experiments operate on strong baselines, which improves the reliability of the experiments.}

To answer question (1), we also explore the \textbf{\emph{full few-shot learning setting}}. Specifically, for intent detection, we randomly
select 5, 10, 20 training instances for each intent to form the few-shot training set. As for slot filling, we also explore 5, 10, 20 shots
settings. However, since one sentence typically involves multiple slots, the number of frequent slot labels may inevitably exceed the
target shot number. To better approximate the target shot number, we select sentences for each slot label sequentially, in the ascending
order of the label frequency.
% As for slot filling, we first sort the slot labels by frequence, and randomly select sentences for the least-frequent slot label first, then the more frequent labels afterwards one by one. Although we also explore 5, 10, 20 shots settings, since one sentence typically involves multiple slots, the number of frequent slot labels may inevitably exceed the target number of shots.
$k_1$-shot dataset is contained by $k_2$-shot dataset if $k_1 < k_2$.
The original test set are used for all settings.
% After that, we move to the adjacent label which is slightly more frequent, and make sure the number of training instances meet the threshold.

Since most few-shot learning methods require either extra classes or classes with enough data for training, we also explore the \textbf{\emph{partial few-shot learning setting}} for intent detection to make fair comparison with existing few-shot learning methods. Specifically, we let the 3 most frequent intents have 300 training instances, and the rest of the few-shot dataset remains untouched.
This is also a common scenario in real world, where we often have several frequent classes and many classes with limited data.

\subsection{RE Patterns}
\label{re_in_exp} The \REs used in our experiments are written by an undergraduate who is familiar with the dataset \orange{following the standard \RE grammar}. It took the
student in total less than 10 hours to develop all the \REs used in the evaluation, but a domain expert can accomplish the task faster. We
use the 20-shot data to develop the \REs, but word lists like citites are obtained from the full training set. The majority of the time
spent on writing the \REs is proportional to the number of \RE groups. Writing an intent \RE takes about 1-2 minutes. In this
work, the student spent 1.5 hours to write the 54 \REs, with on average 2.2 \RE groups for each \RE. The student finds that
writing the slot \RE for the methods described in Sec.~\ref{fusion_with_input} and \ref{fusion_with_output} to be easy, since we only
annotate a simplified version of the slot label. On average, it takes less than two minutes to write an \RE. It took the
student about an hour to write the 60 \REs with on average 1.7 \RE group. 
In contrast to the first two aforementioned
tasks, writing slot \REs to guide attention is more difficult. This is because we need to carefully select clue words and
target to the full slot label. As a result, it took between 2 to 5 minutes to write one \RE, yielding in total 5.5 hours for writing
115 \REs with on average 3.3 \RE groups. The performance of the intent \REs and the slot \REs for the method in
Sec.~\ref{interact_with_module} is shown in Table~\ref{tab_full_few}. \FIXME{ZW: This paragraph needs some attention -- it mixes patterns
and \REs, which is confusing. BTW: Are we following the standard regular expression grammars?}

\FIXME{Luo to ZW: Actually, \emph{pattern} equals to \emph{\RE} in this paper, I have unified this terminology in the paper now. We follow the standard regular expression grammars, which is Perl grammar, but the \REs are written in C++ with boost library.}

% 54 patterns are collected in total
%\orange{(used 1.5 hours)}, with averagely 2.2 (from 1 to 4) \RE groups for each \RE (the group matching a sequence of any words is not
%included). Similarly, writing slot patterns for methods in Sec.~\ref{fusion_with_input} and \ref{fusion_with_output} is also easy since we
%only annotate a simplified version of the slot label. It takes 1-2 minutes to write an \RE, and 60 patterns are produced \orange{(used 70
%minutes)}, with averagely 1.7 (from 1 to 3) \RE groups. However, writing slot patterns to guide attention is more difficult, since we need
%to carefully select informative words and target to the full slot label as well. Typically, writing one pattern requires 2-5 minutes, and
%115 patterns \orange{(used 5.5 hours)} with averagely 3.3 (from 2 to 8) \RE groups are produced. The performance of the intent \texttt{RE}s
%and the slot \texttt{RE}s for the method in Sec.~\ref{interact_with_module} is shown in Table~\ref{tab_full_few}. \FIXME{ZW: We need to
%state in total how long did it take to derive all the REs. State which \RE convention is used. } \orange{LUO to ZW: see the newly added
%orange parts}


\subsection{Experimental Setup}
\paragraph{Hyper-parameters}
We use similar hyper-parameters to \cite{liu2016attention} and achieves comparative results in the original dataset. Specifically, we have batch size 16, dropout probability 0.5, Bi-LSTM size 200 (100 for each direction), attention loss weight 16 (both positive and negative) for few-shot, and weight 1 when we have more data (see Section \ref{sec:experiments} for details). We use 100-dimensional GloVe \cite{pennington2014glove} word vector, and Adam optimizer \cite{kingma2014adam} with learning rate 0.001.

\paragraph{Evaluation Metric}
We report accuracy and macro-F1 for intent detection, and micro/macro-F1 for slot filling.
Micro/macro-F1 are the harmonic mean of micro/macro precision and recall.
Macro-precision/recall are calculated by averaging precision/recall of each label, and micro-precision/recall are averaged over each prediction.
While accuracy and micro statistics show performance on all the instances, macro statitics are more sensitive to classes with limited data.

\paragraph{Naming Conventions}
\texttt{BLSTM} refers to our base model,
\texttt{+feat} means using \RE tag as feature (input side method in Sec.~\ref{fusion_with_input}),
\ptatt refers to the two-side attention method without attention loss,
\texttt{+posi} and \texttt{+neg} refer to using positive and negative attention loss respectively, \texttt{+both} refers to using both attention losses (\NN module side method in Sec.~\ref{interact_with_module}),
\texttt{+logit} means using \RE tag to modify \NN output (output side method in Sec.~\ref{fusion_with_output}),
\texttt{RE} refers to using \RE output as prediction directly\footnote{
For slot filling, \REs for attention guidance are used.},
\texttt{+hu16} means the method of Hu et al.~\shortcite{hu2016harnessing},
\texttt{+mem} means adding the memory module that performs well on few-shot learning \cite{kaiser2017learning}\footnote{
We tune $C$ and $\pi_0$ of \texttt{hu16}, and choose (0.1, 0.3) for intent, and (1, 0.3) for slot. We tune memory-size and $k$ of \texttt{mem}, and choose (1024, 64) for intent, and (2048, 64) for slot.
},
\LL means the joint model \cite{liu2016attention} that achives state-of-art results in ATIS.
