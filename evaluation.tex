\section{Evaluation Methodology}
Our experiments aim to answer two main questions: (1) Does \RE patterns helps? (2) Which combination method works, and when does it work? (3) Does the size of training data influence the improvements? (4) Is our combination method compatible to other more sophisticated \NN architectures?  

\subsection{Datasets}
ATIS (Airline Travel Information Systems) dataset \cite{hemphill1990atis} is a widely used benchmark in \NLU research. It contains audio recordings of people making flight reservations, which involves queries like asking flights, airlines, meal, abbreviations and etc. We follow the setup used by \cite{liu2016attention}, with 4978 queries for training and 893 for test. There are 127 distinct slot labels and 18 different intents. Besides, numbers are also replaced with special tokens like \emph{DIGIT*m}, where m is the number of digits in the original string.

To answer question (3), we also explore a few shot learning setting. Specifically, for intent detection, we randomly select 5, 10, 20 instances from the training set for each intent to form the few-shot learning training set. As for slot filling, since one sentence typically involves multiple slot labels, we can only make the number of training instances for each label as close to the target number as possible. Therefore, we further explore settings with 1, 3, 5, 10, 20 shots. Specifically, we first sort the slot labels by frequence, and randomly select sentence for the least-frequence label first. After that, we move to the adjacent label which is slightly more frequent, and make sure the number of training instances meet the threshold. $k_1$-shot dataset is contained by $k_2$-shot dataset if $k_1 < k_2$, so that the \RE patterns can be reused through different settings. We use the original test set for both intent detection and slot filling.

Since most few-shot learning methods require either extra classes or classes with enough data for training, we also explore a new setting for intent detection to make fair comparison with existing few-shot learning methods. Specifically, we make the 3 most frequent intents to have 300 training instances, and the rest of the few-shot learning dataset remains untouched, so that we can ensure enough training data to make existing few-shot learning methods to work.

The \RE patterns are annotated only based on the few-shot learning datasets, but word lists like city lists are collected using the full dataset. Typically, annotating intent patterns are simple, which takes about 1 minutes to yield one pattern, and we collected 54 patterns in total, and the three combination methods share the same set of patterns. Similarly, annotating slot filling patterns to provide feature or modify output is also easy since it does not require high-precision. It also takes about 1 minutes to yield one pattern, and there are 60 patterns. However, annotating slot filling patterns to guide attention is difficult, since the clue words need careful determination. Tiypically, we need about 2-5 minutes to generate one pattern, and we obtained 115 patterns. These patterns a generated in a single run, and not modified during experiments.


\subsection{Experimental Setup}
\paragraph{Hyper-parameters}
We use similar hyper-parameters to \cite{liu2016attention} and achieves comparative results in the original dataset. Specifically, we have batch size 16, dropout probability 0.5, Bi-LSTM size 200 (100 for each direction), attention loss weight 16 (both positive and negative) for few-shot, and weight 1 when we have more data (see Section \ref{sec:experiments} for details). We use 100-dimensional GloVe \cite{pennington2014glove} word vector trained on Wikipedia and Gigaword, and Adam optimizer \cite{kingma2014adam} with learning rate 0.001.

\paragraph{Evaluation Metric}
Intent: accuracy, macro P, R, F1

Slot: micro/macro P, R, F1

\paragraph{Naming Conventions}
\texttt{raw} refers to attentional Bi-LSTM for intent detection and vanila Bi-LSTM for slot filling, \texttt{two} refers to Bi-LSTM with positive-negative attention (but without attention guidance), \texttt{+feat} means add \RE output as feature, \texttt{+posi} and \texttt{+neg} refers to use positive and negative attention loss respectively, \texttt{+both} refers to use both positive and negative attention, \texttt{RE} refers to using the \RE pattern output as prediction directly, \texttt{mem} means adding the memory module \cite{kaiser2017learning} to \texttt{raw}, \texttt{joint} means the joint model \cite{liu2016attention} which achives state-of-art result in ATIS dataset. 




