\section{Related Work}
\subsection{Few-Shot Learning}
Early work utilizes Bayesian framework to handle few-shot learning problem in a generative manner \cite{fei2006one}. With the prevalence neural network, people begin to consider few-shot learning task in a metric learning framework. The Siamese Network \cite{koch2015siamese} and Matching Network \cite{vinyals2016matching} falls in this category. Another thread of work tries to store rare events in the memory network framework \cite{santoro2016meta, kaiser2017learning}, so that future instances with few-shot classes can retrieve the memory to perform data matching as they do in metric learning based methods.
% Besides, transfer learning technique is also explored in the few-shot learning setting \cite{qiao2017few}.
Our method follows another thread of work that tries to add additional information for few-shot classes. For example, \cite{wang2017multi} explored to use the semantic meaning of the class name itself to guide the attention module in the neural network. Our method, however, seeks to let people derive \RE patterns from existing training data, so that our model can obtain more information to perform prediction. We also show that, our method is compatible to the memory network based methods.

\subsection{Neural Network with Rules}
The recent efforts that tries to fuse \NN and rules and be summarized in 4 aspects. First, rules can lead to better parameter initialization. For example, \cite{li2017initializing} tries to use important n-gram embedding to initialize convolution filters so that CNN can capture the key n-gram in early training stage. Second, in the input part, \cite{wangcombining17} use the ISA relations in knowledge base to find relevant concepts for a short text, and use the concept embeddings to augment input. Third, \cite{hu2016harnessing} also explores to rectify the \NN output by posterior regularization, and let the \NN learn the constraints in a teacher-student framework, and further makes the rules and weights learnable in \cite{hu2016deep}. \cite{guo2017knowledge} followed the same framework and adapted it to the knowledge graph embedding task. \cite{xiao2017symbolic} on the other side, modifies the decoding score of a parsing model by directly multiply a weight derived from rules. Fourth, \cite{demeester2016lifted} modifies the loss function to impose an ordering between the premise and conclusion of first-order logic rules. \cite{alashkar2017examples} modifies the loss by letting the model to fit the label generated by rules along with human-generated labels as well. \cite{xu2017semantic} develops a semantic loss so that the model can learn from legal unlabeled data. Since the fusing in initialization and loss function often have special requirements for the task, which is not applicable to our problem, we explore to fuse \RE patterns in model input, output and use \RE patterns to guide the attention module of a network as well.   

\subsection{Natural Language Understanding}
TBD



