\section{Related Work}

\paragraph{Neural Network with Rules}
Recent efforts that try to fuse \NN and rules and be summarized in 4 aspects:
First, on the initialization side, Li et al.~\shortcite{li2017initializing} uses important n-grams to improve the initialization of convolution filters.
% so that \NN can capture key n-grams in early training stage. 
Second, on the input side, Wang et al.~\shortcite{wang2017combining} uses the ISA relations in knowledge base to find relevant concepts for a short text to augment input.
Third, on the output side, Hu et al.~\shortcite{hu2016harnessing,hu2016deep} and Guo et al.~\shortcite{guo2017knowledge} rectifies the \NN output by posterior regularization, and let \NN learn from the rules in a teacher-student framework. 
% \cite{guo2017knowledge} followed the same framework and adapted it to the knowledge graph embedding task.
Xiao et al.~\shortcite{xiao2017symbolic} modifies the decoding score of a parsing model by directly multiplying a weight derived from rules. 
Fourth, on the loss function side, people modify the loss function to model the relationship between premise and conclusion~\cite{demeester2016lifted}, fit both human-annotated and automatically annotated labels~\cite{alashkar2017examples}, and learn from unlabeled data~\cite{xu2017semantic}.
% \cite{demeester2016lifted} modifies the loss function to impose an ordering between the premise and conclusion of first-order logic rules. \cite{alashkar2017examples} modifies the loss by letting the model to fit the label generated by rules along with human-generated labels as well. \cite{xu2017semantic} develops a semantic loss so that the model can learn from unlabeled data.
Since fusing in initialization and loss function often require special properties of the task, which is not applicable to our problem, we explore to fuse \RE patterns on the input side, \NN module side and output side.   

\paragraph{RE and NN}
As for the specific topic of \RE and \NN, previous work have tried using \RE to speed up the decoding of \NN~\cite{strauss2016regular} and translating natural language queries into \REs~\cite{locascio2016neural}. Our method, on the other side, tries to use \RE to improve the prediction ability of \NN.

\paragraph{Few-Shot Learning}
Early work uses Bayesian framework to handle few-shot learning~\cite{fei2006one}. Recently, people either consider few-shot learning in a metric learning framework~\cite{koch2015siamese,vinyals2016matching}. or store instances in the memory network~\cite{santoro2016meta, kaiser2017learning} to match rare instances in the future.
% Besides, transfer learning technique is also explored in the few-shot learning setting \cite{qiao2017few}.
Our method follows another thread of work that adds additional information to assist few-shot learning.
In this thread, Wang et al.~\shortcite{wang2017multi} uses the semantic meaning of the class name itself to guide the attention module in \NN. 
Our method, however, seeks to use the human-generated \RE to provide additional information.
% We show in the experiments that, our method is compatible to the memory network based methods.

\paragraph{Natural Language Understanding}
Intent detection is a sentence classification problem, and several recurrent neural networks (\texttt{RNN}) have been used~\cite{ravuri2015comparative}.
Slot filling is often treated as a sequence labeling task, and \texttt{RNN} also performs well in this task~\cite{mesnil2015using}. 
People also explore to jointly model the two tasks, and haved achieved better results~\cite{liu2016attention, zhang2016joint}.
% There is also a line of work that tries to use domain adaptation \cite{kim2017domain, liu2017multi, zhu2017concept} to enhance the \NLU performance.



