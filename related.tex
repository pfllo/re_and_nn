\section{Related Work}
Our work builds upon the following techniques, while qualitatively differing from each

 \cparagraph{NN with Rules}
% Recent efforts that try to fuse \NN and rules and be summarized in 4 aspects:
% Recent efforts on fusing \NN and rules can be summarized as follows.
On the initialization side, Li et al.~\shortcite{li2017initializing} uses important n-grams to initialize the convolution filters.
% so that \NN can capture key n-grams in early training stage.
On the input side, Wang et al.~\shortcite{wang2017combining} uses knowledge base rules to find relevant concepts for short texts to augment input.
On the output side,
% the teacher-student framework is widely used to let \NN learn from the posterior label distribution produced by \FOL rules~\cite{hu2016harnessing,hu2016deep,guo2017knowledge}.
Hu et al.~\shortcite{hu2016harnessing,hu2016deep} and Guo et al.~\shortcite{guo2017knowledge} use \FOL rules to rectify the output probability of \NN, and then let \NN learn from the rectified distribution in a teacher-student framework.
% \cite{guo2017knowledge} followed the same framework and adapted it to the knowledge graph embedding task.
Xiao et al.~\shortcite{xiao2017symbolic}, on the other hand, modifies the decoding score of \NN by multiplying a weight derived from rules.
On the loss function side, people modify the loss function to model the relationship between premise and conclusion~\cite{demeester2016lifted}, and fit both human-annotated and rule-annotated labels~\cite{alashkar2017examples}.
% and learn from unlabeled data~\cite{xu2017semantic}.
% \cite{demeester2016lifted} modifies the loss function to impose an ordering between the premise and conclusion of first-order logic rules. \cite{alashkar2017examples} modifies the loss by letting the model to fit the label generated by rules along with human-generated labels as well. \cite{xu2017semantic} develops a semantic loss so that the model can learn from unlabeled data.
Since fusing in initialization and loss function often require special properties of the task, which is not applicable to our problem.
Instead, explore to combine \RE rules with different levels of a \NN.

\cparagraph{NNs and REs} As for \NNs and \REs, previous work has tried to use \RE to speed up the decoding phase of a
\NN~\cite{strauss2016regular} and generating \REs from natural language specifications of the \RE~\cite{locascio2016neural}. By constrast,
our work aims to use \REs to improve the prediction ability of a \NN.

\cparagraph{Few-Shot Learning}
% Early work uses Bayesian framework to handle few-shot learning~\cite{fei2006one}.
Prior work either considers few-shot learning in a metric learning framework~\cite{koch2015siamese,vinyals2016matching}, or stores
instances in a memory~\cite{santoro2016meta, kaiser2017learning} to match similar instances in the future.
% Besides, transfer learning technique is also explored in the few-shot learning setting \cite{qiao2017few}.
% Our method follows another thread of work that adds additional information to assist few-shot learning.
% In this thread, Wang et al.~\shortcite{wang2017multi} uses the semantic meaning of the class name itself to guide the attention module in \NN.
Wang et al.~\shortcite{wang2017multi} further uses the semantic meaning of the class name itself to provide extra information for few-shot
learning. Unlike these previous studies, we seek to use the human-generated \REs to provide additional information.
% We show in the experiments that, our method is compatible to the memory network based methods.

\cparagraph{Natural Language Understanding}
Recurrent neural networks (\texttt{RNN}) are proven to be effective in both intent detection~\cite{ravuri2015comparative} and slot filling~\cite{mesnil2015using}.
% Intent detection is a sentence classification problem, and several recurrent neural networks (\texttt{RNN}) have been used~\cite{ravuri2015comparative}.
% Slot filling is often treated as a sequence labeling task, and \texttt{RNN} also performs well in this task~\cite{mesnil2015using}.
Researchers also explore ways to jointly model the two tasks~\cite{liu2016attention, zhang2016joint}. However, no work so far has combined
\REs and \NNs to improve intent detection and slot filling. %Our work the first to do so.
% There is also a line of work that tries to use domain adaptation \cite{kim2017domain, liu2017multi, zhu2017concept} to enhance the \NLU performance.
