\section{Related Work}

\cparagraph{NN with Rules}
% Recent efforts that try to fuse \NN and rules and be summarized in 4 aspects:
% Recent efforts on fusing \NN and rules can be summarized as follows.
On the initialization side, Li et al.~\shortcite{li2017initializing} uses important n-grams to initialize the convolution filters. 
% so that \NN can capture key n-grams in early training stage. 
On the input side, Wang et al.~\shortcite{wang2017combining} uses knowledge base rules to find relevant concepts for short texts to augment input.
On the output side,
% the teacher-student framework is widely used to let \NN learn from the posterior label distribution produced by \FOL rules~\cite{hu2016harnessing,hu2016deep,guo2017knowledge}.
Hu et al.~\shortcite{hu2016harnessing,hu2016deep} and Guo et al.~\shortcite{guo2017knowledge} use \FOL rules to rectify the output probability of \NN, and then let \NN learn from the rectified distribution in a teacher-student framework.
% \cite{guo2017knowledge} followed the same framework and adapted it to the knowledge graph embedding task.
Xiao et al.~\shortcite{xiao2017symbolic}, on the other hand, modifies the decoding score of \NN by multiplying a weight derived from rules.
On the loss function side, people modify the loss function to model the relationship between premise and conclusion~\cite{demeester2016lifted}, and fit both human-annotated and rule-annotated labels~\cite{alashkar2017examples}.
% and learn from unlabeled data~\cite{xu2017semantic}.
% \cite{demeester2016lifted} modifies the loss function to impose an ordering between the premise and conclusion of first-order logic rules. \cite{alashkar2017examples} modifies the loss by letting the model to fit the label generated by rules along with human-generated labels as well. \cite{xu2017semantic} develops a semantic loss so that the model can learn from unlabeled data.
Since fusing in initialization and loss function often require special properties of the task, which is not applicable to our problem, we explore to fuse \RE rules on the input, \NN module and output side.   

\cparagraph{NN and RE}
As for \NN and \RE, previous work have tried using \RE to speed up the decoding of \NN~\cite{strauss2016regular} and translating natural language queries into \REs~\cite{locascio2016neural}.
% Our work, on the other side, tries to use \RE to improve the prediction ability of \NN.

\cparagraph{Few-Shot Learning}
% Early work uses Bayesian framework to handle few-shot learning~\cite{fei2006one}.
Recently, people either consider few-shot learning in a metric learning framework~\cite{koch2015siamese,vinyals2016matching}, or store instances in a memory~\cite{santoro2016meta, kaiser2017learning} to match rare instances in the future.
% Besides, transfer learning technique is also explored in the few-shot learning setting \cite{qiao2017few}.
% Our method follows another thread of work that adds additional information to assist few-shot learning.
% In this thread, Wang et al.~\shortcite{wang2017multi} uses the semantic meaning of the class name itself to guide the attention module in \NN.
Wang et al.~\shortcite{wang2017multi} further uses the semantic meaning of the class name itself to provide extra information for few-shot learning.
Our method, however, seeks to use the human-generated \RE to provide additional information.
% We show in the experiments that, our method is compatible to the memory network based methods.

\cparagraph{Natural Language Understanding}
Recurrent neural networks (\texttt{RNN}) are proven to be effective in both intent detection~\cite{ravuri2015comparative} and slot filling~\cite{mesnil2015using}.
% Intent detection is a sentence classification problem, and several recurrent neural networks (\texttt{RNN}) have been used~\cite{ravuri2015comparative}.
% Slot filling is often treated as a sequence labeling task, and \texttt{RNN} also performs well in this task~\cite{mesnil2015using}. 
Further, people explore to jointly model the two tasks, and have achieved better results~\cite{liu2016attention, zhang2016joint}.
% There is also a line of work that tries to use domain adaptation \cite{kim2017domain, liu2017multi, zhu2017concept} to enhance the \NLU performance.



